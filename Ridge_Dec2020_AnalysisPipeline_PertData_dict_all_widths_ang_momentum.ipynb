{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from math import acos, degrees\n",
    "from scipy.signal import find_peaks\n",
    "import os.path\n",
    "import glob\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import entropy\n",
    "import pylab as pl\n",
    "from numpy.fft import fft\n",
    "from scipy import stats\n",
    "import numpy\n",
    "from scipy import signal\n",
    "from scipy.signal import lfilter, lfilter_zi, filtfilt, butter\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_width = 200\n",
    "#centr_rang = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract angle using 3 points coordinate\n",
    "def angle3pt(a, b, c):\n",
    "#    \"\"\"Counterclockwise angle in degrees by turning from c to a around b\n",
    "#        Returns a float between 0.0 and 360.0\"\"\"\n",
    "    ang = math.degrees(\n",
    "    math.atan2(c[1]-b[1], c[0]-b[0]) - math.atan2(a[1]-b[1], a[0]-b[0]))\n",
    "    return ang + 360 if ang < 0 else ang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getridofAngleJumps(alpha):\n",
    "    alpha_rad = [x*(np.pi)/180 for x in alpha]\n",
    "    alpha_rad = np.array(alpha_rad)\n",
    "    alpha_rad[~np.isnan(alpha_rad)] = np.unwrap(alpha_rad[~np.isnan(alpha_rad)])\n",
    "    alpha_unwrap= np.degrees(alpha_rad)\n",
    "    return alpha_unwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_acausal(x,frequency = 0.300):\n",
    "    #b, a = signal.butter(8, 0.150)\n",
    "    sos = signal.butter(4, frequency, output='sos')\n",
    "    y = signal.sosfiltfilt(sos, x)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x,window_len=1,window='hanning'):\n",
    "#    \"\"\"smooth the data using a window with requested size.\n",
    "#    \n",
    "#    This method is based on the convolution of a scaled window with the signal.\n",
    "#    The signal is prepared by introducing reflected copies of the signal \n",
    "#    (with the window size) in both ends so that transient parts are minimized\n",
    "#    in the begining and end part of the output signal.\n",
    "#    \n",
    "#    input:\n",
    "#        x: the input signal \n",
    "#        window_len: the dimension of the smoothing window; should be an odd integer\n",
    "#        window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
    "#            flat window will produce a moving average smoothing.\n",
    "#\n",
    "#    output:\n",
    "#        the smoothed signal\n",
    "#        \n",
    "#    example:\n",
    "#\n",
    "#    t=linspace(-2,2,0.1)\n",
    "#    y=smooth(x)\n",
    "#    \n",
    "#    \n",
    "#    numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve\n",
    "#    scipy.signal.lfilter\n",
    " \n",
    "#    TODO: the window parameter could be the window itself if an array instead of a string\n",
    "#    NOTE: length(output) != length(input), to correct this: return y[(window_len/2-1):-(window_len/2)] instead of just y.\n",
    "\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "\n",
    "    if x.size < window_len:\n",
    "        raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
    "\n",
    "\n",
    "    if window_len<3:\n",
    "        return x\n",
    "\n",
    "\n",
    "    if not window in ['flat', 'hanning', 'hamming', 'bartlett', 'blackman']:\n",
    "        raise ValueError(\"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\")\n",
    "\n",
    "\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-2:-window_len-1:-1]]\n",
    "    #print(len(s))\n",
    "    if window == 'flat': #moving average\n",
    "        w=np.ones(window_len,'d')\n",
    "    else:\n",
    "        w=eval('numpy.'+window+'(window_len)')\n",
    "\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract RidgeX trajectory from excel file\n",
    "def RidgeX_excel_to_array_preprocessed(file_path, chunk_width, trial_no):\n",
    "    #Read csv file tail markers\n",
    "    RidgeX = pd.read_csv(file_path[trial_no])\n",
    "\n",
    "    #take just numeric values\n",
    "    RidgeX=pd.to_numeric(RidgeX.iloc[:,0])\n",
    "\n",
    " \n",
    "    return smooth(RidgeX.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot tail angle trajectory\n",
    "def plot_TailAngle(file_path, chunk_width, i, c):\n",
    "    #Read csv file tail markers\n",
    "    df = pd.read_csv(file_path[i])\n",
    "#    df = pd.read_csv(file_path)\n",
    "\n",
    "    #Rename marker columns\n",
    "    df.rename(columns={'DLC_resnet50_Ridge_walkMay27shuffle1_200000':'tail1_x',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.1':'tail1_y',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.2':'tail1_lik',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.3':'tail2_x',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.4':'tail2_y',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.5':'tail2_lik',                       \n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.6':'tail3_x',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.7':'tail3_y',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.8':'tail3_lik'}, \n",
    "                 inplace=True)\n",
    "\n",
    "    #take just numeric values\n",
    "    df.tail1_x=pd.to_numeric(df.tail1_x[c-chunk_width:c+chunk_width])\n",
    "    df.tail1_y=pd.to_numeric(df.tail1_y[c-chunk_width:c+chunk_width])\n",
    "    df.tail2_x=pd.to_numeric(df.tail2_x[c-chunk_width:c+chunk_width])\n",
    "    df.tail2_y=pd.to_numeric(df.tail2_y[c-chunk_width:c+chunk_width])\n",
    "    df.tail3_x=pd.to_numeric(df.tail3_x[c-chunk_width:c+chunk_width])\n",
    "    df.tail3_y=pd.to_numeric(df.tail3_y[c-chunk_width:c+chunk_width])\n",
    "    df.tail1_lik=pd.to_numeric(df.tail1_lik[c-chunk_width:c+chunk_width])\n",
    "    df.tail2_lik=pd.to_numeric(df.tail2_lik[c-chunk_width:c+chunk_width])\n",
    "    df.tail3_lik=pd.to_numeric(df.tail3_lik[c-chunk_width:c+chunk_width])\n",
    "    \n",
    "    #substitute low likelihood points with NaN\n",
    "    #df.tail1_x[]=np.nan\n",
    "    lik_thresh = 0.05\n",
    "    df.tail1_x.where((df.tail1_lik>lik_thresh),np.NaN,inplace=True)\n",
    "    df.tail1_y.where((df.tail1_lik>lik_thresh),np.NaN,inplace=True)\n",
    "    df.tail2_x.where((df.tail2_lik>lik_thresh),np.NaN,inplace=True)\n",
    "    df.tail2_y.where((df.tail2_lik>lik_thresh),np.NaN,inplace=True)\n",
    "    df.tail3_x.where((df.tail3_lik>lik_thresh),np.NaN,inplace=True)\n",
    "    df.tail3_y.where((df.tail3_lik>lik_thresh),np.NaN,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Compute and plot tail angle in panda dataframe as last column\n",
    "    angles=[]\n",
    "    for i in range(df.shape[0]):\n",
    "        #x1,y1=df.tail1_x[i],df.tail1_y[i]\n",
    "        vertical = np.array([df.tail1_x[i],df.tail1_y[i]+10])\n",
    "        tail1 = np.array([df.tail1_x[i],df.tail1_y[i]])\n",
    "        tail2 = np.array([df.tail2_x[i],df.tail2_y[i]])    \n",
    "        tail3 = np.array([df.tail3_x[i],df.tail3_y[i]])\n",
    "    \n",
    "\n",
    "    #Change below to decide 3 points to determine angle\n",
    "        angle = angle3pt(tail2, tail1, vertical)\n",
    "        #Append\n",
    "        angles.append(round(angle,2))\n",
    "    df['Angles']=angles\n",
    "    df.head()\n",
    "    \n",
    "    #Create a list comprehension by chunking x (centroid X trajectory) in 'chunk_width' frames and alpha (tail angle traj)\n",
    "    #x = CentroidXY.X\n",
    "    alpha = df.Angles\n",
    "    #Get alpha value at perturbation time to centre the trace to that value\n",
    "#    alpha_centred = alpha[tot_peaks]\n",
    "    #Apply function to get rid of angle jumps\n",
    "    alpha = getridofAngleJumps(alpha)\n",
    "    #Apply function to smooth\n",
    "    alpha = smooth(alpha)\n",
    "#    TailAngle_traj = alpha[tot_peaks-chunk_width:tot_peaks+chunk_width]-[alpha[tot_peaks]-alpha_centred]# for i in tot_peaks]\n",
    "#!    return [TailAngle_traj, alpha]\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot Right Paw angle trajectory\n",
    "def plot_PawAngle(file_path, chunk_width, trial_no):\n",
    "    #Read csv file tail markers\n",
    "    df = pd.read_csv(file_path[trial_no])\n",
    "#    df = pd.read_csv(file_path)\n",
    "\n",
    "    #Rename marker columns\n",
    "    df.rename(columns={'DLC_resnet50_Ridge_walkMay27shuffle1_200000.9':'LP_x',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.10':'LP_y',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.11':'LP_lik',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.24':'RA_x',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.25':'RA_y',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.26':'RA_lik',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.21':'RP_x',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.22':'RP_y',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.23':'RP_lik'}, \n",
    "                 inplace=True)\n",
    "\n",
    "    #take just numeric values\n",
    "    df.RA_x=pd.to_numeric(df.RA_x[2:])\n",
    "    df.RA_y=pd.to_numeric(df.RA_y[2:])\n",
    "    df.RP_x=pd.to_numeric(df.RP_x[2:])\n",
    "    df.RP_y=pd.to_numeric(df.RP_y[2:])\n",
    "    df.LP_x=pd.to_numeric(df.LP_x[2:])\n",
    "    df.LP_y=pd.to_numeric(df.LP_y[2:])\n",
    "    df.RA_lik=pd.to_numeric(df.RA_lik[2:])\n",
    "    df.RP_lik=pd.to_numeric(df.RP_lik[2:])\n",
    "    df.LP_lik=pd.to_numeric(df.LP_lik[2:])\n",
    "\n",
    "    \n",
    "    #substitute low likelihood points with NaN\n",
    "    #df.tail1_x[]=np.nan\n",
    "    lik_thresh = 0.1\n",
    "    df.RA_x.where((df.RA_lik>lik_thresh),np.NaN,inplace=True)\n",
    "    df.RA_y.where((df.RA_lik>lik_thresh),np.NaN,inplace=True)\n",
    "    df.RP_x.where((df.RP_lik>lik_thresh),np.NaN,inplace=True)\n",
    "    df.RP_y.where((df.RP_lik>lik_thresh),np.NaN,inplace=True)\n",
    "    df.LP_x.where((df.LP_lik>lik_thresh),np.NaN,inplace=True)\n",
    "    df.LP_y.where((df.LP_lik>lik_thresh),np.NaN,inplace=True)\n",
    "    \n",
    "    #extract RP x and LP x\n",
    "    RP_x =  df.RP_x\n",
    "    LP_x =  df.LP_x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Compute and plot tail angle in panda dataframe as last column\n",
    "    angles=[]\n",
    "    for i in range(df.shape[0]):\n",
    "        vertical = np.array([df.RA_x[i],df.RA_y[i]+10])\n",
    "        RA = np.array([df.RA_x[i],df.RA_y[i]])\n",
    "        RP = np.array([df.RP_x[i],df.RP_y[i]])\n",
    "\n",
    "        angle = angle3pt(RP, RA, vertical)\n",
    "        #Append\n",
    "        angles.append(round(angle,2))\n",
    "    df['Angles']=angles\n",
    "    df.head()\n",
    "    \n",
    "    #Create a list comprehension by chunking x (centroid X trajectory) in 'chunk_width' frames and alpha (tail angle traj)\n",
    "    #x = CentroidXY.X\n",
    "    alpha = df.Angles\n",
    "    #Get alpha value at perturbation time to centre the trace to that value\n",
    "#    alpha_centred = alpha[tot_peaks]\n",
    "    #Apply function to get rid of angle jumps\n",
    "    alpha = getridofAngleJumps(alpha)\n",
    "    #Apply function to smooth\n",
    "    alpha = smooth(alpha)\n",
    "#    TailAngle_traj = alpha[tot_peaks-chunk_width:tot_peaks+chunk_width]-[alpha[tot_peaks]-alpha_centred]# for i in tot_peaks]\n",
    "#    RP_x = RP_x[tot_peaks-chunk_width:tot_peaks+chunk_width]-[RP_x[tot_peaks]]\n",
    "#    LP_x = LP_x[tot_peaks-chunk_width:tot_peaks+chunk_width]-[LP_x[tot_peaks]]\n",
    "\n",
    "#!    return [TailAngle_traj, alpha]\n",
    "    return [alpha, smooth(RP_x), smooth(LP_x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function to plot Right Paw angle trajectory\n",
    "# def plot_LPAngle(file_path, chunk_width, trial_no):\n",
    "#     #Read csv file tail markers\n",
    "#     df = pd.read_csv(file_path[trial_no])\n",
    "# #    df = pd.read_csv(file_path)\n",
    "\n",
    "#     #Rename marker columns\n",
    "#     df.rename(columns={'DLC_resnet50_Ridge_walkMay27shuffle1_200000.9':'LP_x',\n",
    "#                           'DLC_resnet50_Ridge_walkMay27shuffle1_200000.10':'LP_y',\n",
    "#                           'DLC_resnet50_Ridge_walkMay27shuffle1_200000.11':'LP_lik',\n",
    "#                           'DLC_resnet50_Ridge_walkMay27shuffle1_200000.12':'LA_x',\n",
    "#                           'DLC_resnet50_Ridge_walkMay27shuffle1_200000.13':'LA_y',\n",
    "#                           'DLC_resnet50_Ridge_walkMay27shuffle1_200000.14':'LA_lik',\n",
    "                       \n",
    "#                           'DLC_resnet50_Ridge_walkMay27shuffle1_200000.24':'RA_x',\n",
    "#                           'DLC_resnet50_Ridge_walkMay27shuffle1_200000.25':'RA_y',\n",
    "#                           'DLC_resnet50_Ridge_walkMay27shuffle1_200000.26':'RA_lik',\n",
    "#                           'DLC_resnet50_Ridge_walkMay27shuffle1_200000.21':'RP_x',\n",
    "#                           'DLC_resnet50_Ridge_walkMay27shuffle1_200000.22':'RP_y',\n",
    "#                           'DLC_resnet50_Ridge_walkMay27shuffle1_200000.23':'RP_lik'}, \n",
    "#                  inplace=True)\n",
    "\n",
    "#     #take just numeric values\n",
    "#     df.RA_x=pd.to_numeric(df.RA_x[2:])\n",
    "#     df.LA_x=pd.to_numeric(df.LA_x[2:])\n",
    "\n",
    "#     df.RA_y=pd.to_numeric(df.RA_y[2:])\n",
    "#     df.LA_y=pd.to_numeric(df.LA_y[2:])\n",
    "\n",
    "#     df.RP_x=pd.to_numeric(df.RP_x[2:])\n",
    "#     df.RP_y=pd.to_numeric(df.RP_y[2:])\n",
    "#     df.LP_x=pd.to_numeric(df.LP_x[2:])\n",
    "#     df.LP_y=pd.to_numeric(df.LP_y[2:])\n",
    "    \n",
    "#     df.RA_lik=pd.to_numeric(df.RA_lik[2:])\n",
    "#     df.LA_lik=pd.to_numeric(df.LA_lik[2:])\n",
    "#     df.RP_lik=pd.to_numeric(df.RP_lik[2:])\n",
    "#     df.LP_lik=pd.to_numeric(df.LP_lik[2:])\n",
    "\n",
    "    \n",
    "#     #substitute low likelihood points with NaN\n",
    "#     #df.tail1_x[]=np.nan\n",
    "#     lik_thresh = 0.1\n",
    "#     df.RA_x.where((df.RA_lik>lik_thresh),np.NaN,inplace=True)\n",
    "#     df.LA_x.where((df.LA_lik>lik_thresh),np.NaN,inplace=True)\n",
    "\n",
    "#     df.RA_y.where((df.RA_lik>lik_thresh),np.NaN,inplace=True)\n",
    "#     df.LA_y.where((df.LA_lik>lik_thresh),np.NaN,inplace=True)\n",
    "\n",
    "#     df.RP_x.where((df.RP_lik>lik_thresh),np.NaN,inplace=True)\n",
    "#     df.RP_y.where((df.RP_lik>lik_thresh),np.NaN,inplace=True)\n",
    "#     df.LP_x.where((df.LP_lik>lik_thresh),np.NaN,inplace=True)\n",
    "#     df.LP_y.where((df.LP_lik>lik_thresh),np.NaN,inplace=True)\n",
    "    \n",
    "#     #extract RP x and LP x\n",
    "#     RP_x =  df.RP_x\n",
    "#     LP_x =  df.LP_x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #Compute and plot tail angle in panda dataframe as last column\n",
    "#     angles=[]\n",
    "#     for i in range(df.shape[0]):\n",
    "#         vertical = np.array([df.LA_x[i],df.LA_y[i]+10])\n",
    "#         LA = np.array([df.LA_x[i],df.LA_y[i]])\n",
    "#         LP = np.array([df.LP_x[i],df.LP_y[i]])\n",
    "\n",
    "#         angle = angle3pt(LP, LA, vertical)\n",
    "#         #Append\n",
    "#         angles.append(round(angle,2))\n",
    "#     df['Angles']=angles\n",
    "#     df.head()\n",
    "    \n",
    "#     #Create a list comprehension by chunking x (centroid X trajectory) in 'chunk_width' frames and alpha (tail angle traj)\n",
    "#     #x = CentroidXY.X\n",
    "#     alpha = df.Angles\n",
    "#     #Get alpha value at perturbation time to centre the trace to that value\n",
    "# #    alpha_centred = alpha[tot_peaks]\n",
    "#     #Apply function to get rid of angle jumps\n",
    "#     alpha = getridofAngleJumps(alpha)\n",
    "#     #Apply function to smooth\n",
    "#     alpha = smooth(alpha)\n",
    "# #    TailAngle_traj = alpha[tot_peaks-chunk_width:tot_peaks+chunk_width]-[alpha[tot_peaks]-alpha_centred]# for i in tot_peaks]\n",
    "# #    RP_x = RP_x[tot_peaks-chunk_width:tot_peaks+chunk_width]-[RP_x[tot_peaks]]\n",
    "# #    LP_x = LP_x[tot_peaks-chunk_width:tot_peaks+chunk_width]-[LP_x[tot_peaks]]\n",
    "\n",
    "# #!    return [TailAngle_traj, alpha]\n",
    "#     return [alpha, smooth(RP_x), smooth(LP_x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot Right Paw angle trajectory\n",
    "def plot_HipAngle(file_path, chunk_width, trial_no, c):\n",
    "    #Read csv file tail markers\n",
    "    df = pd.read_csv(file_path[trial_no])\n",
    "#    df = pd.read_csv(file_path)\n",
    "\n",
    "    #Rename marker columns\n",
    "    df.rename(columns={'DLC_resnet50_Ridge_walkMay27shuffle1_200000.15':'LH_x',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.16':'LH_y',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.17':'LH_lik',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.18':'RH_x',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.19':'RH_y',\n",
    "                          'DLC_resnet50_Ridge_walkMay27shuffle1_200000.20':'RH_lik'}, \n",
    "                 inplace=True)\n",
    "\n",
    "    #take just numeric values\n",
    "    df.LH_x=pd.to_numeric(df.LH_x[c-chunk_width:c+chunk_width])\n",
    "    df.LH_y=pd.to_numeric(df.LH_y[c-chunk_width:c+chunk_width])\n",
    "    df.RH_x=pd.to_numeric(df.RH_x[c-chunk_width:c+chunk_width])\n",
    "    df.RH_y=pd.to_numeric(df.RH_y[c-chunk_width:c+chunk_width])\n",
    "    df.RH_lik=pd.to_numeric(df.RH_lik[c-chunk_width:c+chunk_width])\n",
    "    df.LH_lik=pd.to_numeric(df.LH_lik[c-chunk_width:c+chunk_width])\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    #substitute low likelihood points with NaN\n",
    "    #df.tail1_x[]=np.nan\n",
    "    lik_thresh = 0.1\n",
    "    df.LH_x.where((df.LH_lik>lik_thresh),np.NaN,inplace=True)\n",
    "    df.LH_y.where((df.LH_lik>lik_thresh),np.NaN,inplace=True)\n",
    "    df.RH_x.where((df.RH_lik>lik_thresh),np.NaN,inplace=True)\n",
    "    df.RH_y.where((df.RH_lik>lik_thresh),np.NaN,inplace=True)\n",
    "\n",
    "    \n",
    "    #extract RP x and LP x\n",
    "    LH_x =  df.LH_x\n",
    "    LH_y =  df.LH_y\n",
    "    RH_x =  df.RH_x    \n",
    "    RH_y =  df.RH_y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Compute and plot tail angle in panda dataframe as last column\n",
    "    angles=[]\n",
    "    for i in range(df.shape[0]):\n",
    "        vertical = np.array([df.RH_x[i],df.RH_y[i]+10])\n",
    "        RH = np.array([df.RH_x[i],df.RH_y[i]])\n",
    "        LH = np.array([df.LH_x[i],df.LH_y[i]])\n",
    "\n",
    "        angle = angle3pt(LH, RH, vertical)\n",
    "        #Append\n",
    "        angles.append(round(angle,2))\n",
    "    df['Angles']=angles\n",
    "    df.head()\n",
    "    \n",
    "    #Create a list comprehension by chunking x (centroid X trajectory) in 'chunk_width' frames and alpha (tail angle traj)\n",
    "    #x = CentroidXY.X\n",
    "    alpha = df.Angles\n",
    "    #Get alpha value at perturbation time to centre the trace to that value\n",
    "#    alpha_centred = alpha[tot_peaks]\n",
    "    #Apply function to get rid of angle jumps\n",
    "    alpha = getridofAngleJumps(alpha)\n",
    "    #Apply function to smooth\n",
    "    alpha = smooth(alpha)\n",
    "#    TailAngle_traj = alpha[tot_peaks-chunk_width:tot_peaks+chunk_width]-[alpha[tot_peaks]-alpha_centred]# for i in tot_peaks]\n",
    "#    RP_x = RP_x[tot_peaks-chunk_width:tot_peaks+chunk_width]-[RP_x[tot_peaks]]\n",
    "#    LP_x = LP_x[tot_peaks-chunk_width:tot_peaks+chunk_width]-[LP_x[tot_peaks]]\n",
    "\n",
    "#!    return [TailAngle_traj, alpha]\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract centroid X Y trajectory\n",
    "def extract_Centroid(file_path, chunk_width, trial_no):\n",
    "    #Read csv file tail markers\n",
    "    CentroidXY = pd.read_csv(file_path[trial_no])\n",
    "\n",
    "    CentroidXY.rename(columns={'NaN':'X',\n",
    "                              'NaN.1':'Y'}, \n",
    "                     inplace=True)\n",
    "    #take just numeric values\n",
    "    CentroidXY.Centroid_x=pd.to_numeric(CentroidXY.X)\n",
    "    CentroidXY.Centroid_y=pd.to_numeric(CentroidXY.Y)\n",
    "\n",
    "    #extract Centroid x and y\n",
    "    CentroidX =  CentroidXY.Centroid_x.values\n",
    "    CentroidY =  CentroidXY.Centroid_y.values\n",
    "    CentroidX = [el for el in CentroidX]\n",
    "    CentroidY = [el for el in CentroidY]\n",
    "\n",
    "    return np.array(CentroidX).ravel(), np.array(CentroidY).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_delay(a, b):\n",
    "    corr_a_b = np.correlate(a-np.mean(a), b-np.mean(b), mode = 'full')\n",
    "    delay = np.where(corr_a_b == numpy.amin(corr_a_b))# -(np.size(corr_a_b)+1)/2\n",
    "    return delay[0]-(np.size(corr_a_b)+1)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot Right Paw angle trajectory\n",
    "def plot_Centroid_edge_dist(file_path, chunk_width, i):\n",
    "    #Read csv file tail markers\n",
    "    df = pd.read_csv(file_path[i])\n",
    "\n",
    "##Open Centroid file from top camera\n",
    "#CentroidXY = pd.read_csv('C:/Users/Salvo/Desktop/Ridge/DLC_videos/Videos_to_analyzeDLC/Ridge_MiceS20-S24_16thApril/perturbation_8mm_1/8_S22/Centroid.csv')\n",
    "\n",
    "    #Select 1st column csv file\n",
    "    matrix2 = df[df.columns[0]]#.as_matrix()\n",
    "    Centroid1stcol = matrix2.tolist() #file 1st column\n",
    "\n",
    "\n",
    "#    CentroidXY.rename(columns={'NaN':'dist'}, \n",
    "#                     inplace=True)\n",
    "    #take just numeric values\n",
    "    Centroid1stcol = pd.to_numeric(Centroid1stcol)\n",
    "#    CentroidXY.Centroid_y=pd.to_numeric(CentroidXY.Y)\n",
    "\n",
    "    #extract Centroid x and y\n",
    "#    CentroidX =  CentroidXY.Centroid_x\n",
    "#    CentroidY =  CentroidXY.Centroid_y\n",
    "    \n",
    "#    Centroid_list = CentroidX[tot_peaks-chunk_width:tot_peaks+centr_rang-chunk_width]#-[CentroidX[tot_peaks]]\n",
    "    \n",
    "#    Centroid_list = CentroidX[tot_peaks-chunk_width-100:tot_peaks-100]-[CentroidX[tot_peaks-100]]\n",
    "    return smooth(Centroid1stcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#np.arange(len(file_to_open)-25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def firstNonNan(listfloats):\n",
    "    i = 0\n",
    "    for item in listfloats:\n",
    "        i += 1\n",
    "        if math.isnan(item) == False:\n",
    "            return i\n",
    "\n",
    "#firstNonNan(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HIST_MI_2_var(a, b):\n",
    "    fig = plt.figure(figsize=(10,14))\n",
    "    ax1 = plt.subplot(311)\n",
    "    ax2 = plt.subplot(312)\n",
    "    hist_centr = ax1.hist(a, density=True, bins=30, color = 'orange')  # `density=False` would make counts\n",
    "    hist_tail = ax2.hist(b, density=True, bins=30, color = 'blue')  # `density=False` would make counts\n",
    "#    ent_cent = entropy(hist_centr[0], base=2)\n",
    "#    ent_tail = entropy(hist_tail[0], base=2)\n",
    "    MI_cent_tail = metrics.mutual_info_score(hist_centr[0], hist_tail[0])\n",
    "    return MI_cent_tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_delay_array(var1, var2):\n",
    "    var1 = var1[~np.isnan(var1)] #centroid\n",
    "    var2 = var2[~np.isnan(var2)] #TA\n",
    "    #take the mean out\n",
    "#    var1 = var1-np.mean(var1)\n",
    "#    var2 = var2-np.mean(var2)\n",
    "\n",
    "    corr_a_b = np.correlate(var2, var1, mode = 'full')\n",
    "    norm_corr_a_b = np.correlate(var2/np.std(var2), var1/np.std(var1), mode = 'full')\n",
    "    cc_trace_midpoint = len(norm_corr_a_b)\n",
    "    delay = np.argmax(abs(norm_corr_a_b))-(cc_trace_midpoint/2)+1 #Get the delay of the absolute max peak\n",
    "    max_peak = max(norm_corr_a_b, key=abs)\n",
    "#    max_peak = abs(max(corr_a_b, key=abs))\n",
    "    return delay, max_peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fft(chunk_width, data):\n",
    "    # Number of sample points\n",
    "    N = chunk_width*2\n",
    "    # sample spacing\n",
    "    T = 1/300\n",
    "    x = np.linspace(0.0, N*T, N)\n",
    "    y = data\n",
    "    yf = fft(y)\n",
    "    xf = np.linspace(0, 1/(2*T), N//2)\n",
    "#    plt.plot(xf, 2/N * np.abs(yf[0:N//2]))\n",
    "#    plt.grid()\n",
    "#    plt.show()\n",
    "    return xf, yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findpeaks_extractchunk(x_diff, x, y, z, w, t, threshold_height, chunk_width_step):\n",
    "    peaks, _ = find_peaks(x_diff, height=threshold_height)\n",
    "    out_step = []\n",
    "    out_TA = []\n",
    "    out_HA = []\n",
    "    out_cent = []\n",
    "    out_RstepAng = []\n",
    "\n",
    "    for i in np.arange(len(peaks)):\n",
    "        chunk_trial_step = x[peaks[i]-chunk_width_step:peaks[i]+chunk_width_step]\n",
    "        chunk_trial_TA = y[peaks[i]-chunk_width_step:peaks[i]+chunk_width_step]\n",
    "        chunk_trial_HA = z[peaks[i]-chunk_width_step:peaks[i]+chunk_width_step]\n",
    "        chunk_trial_cent = w[peaks[i]-chunk_width_step:peaks[i]+chunk_width_step]\n",
    "        chunk_trial_RstepAng = t[peaks[i]-chunk_width_step:peaks[i]+chunk_width_step]\n",
    "        \n",
    "\n",
    "        out_step.append(chunk_trial_step)\n",
    "        out_TA.append(chunk_trial_TA)\n",
    "        out_HA.append(chunk_trial_HA)\n",
    "        out_cent.append(chunk_trial_cent)\n",
    "        #transpose all traces of step angle greater than 360 back to 0\n",
    "        if np.nanmean(chunk_trial_RstepAng) > 250:\n",
    "            out_RstepAng.append(chunk_trial_RstepAng-360)\n",
    "        elif np.nanmean(chunk_trial_RstepAng) < -150:\n",
    "            out_RstepAng.append(chunk_trial_RstepAng+360)   \n",
    "        else:\n",
    "            out_RstepAng.append(chunk_trial_RstepAng)\n",
    "\n",
    "\n",
    "    \n",
    "    return out_step, out_TA, out_HA, out_cent, out_RstepAng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.text as mpl_text\n",
    "\n",
    "class AnyObject(object):\n",
    "    def __init__(self, text, color):\n",
    "        self.my_text = text\n",
    "        self.my_color = color\n",
    "\n",
    "class AnyObjectHandler(object):\n",
    "    def legend_artist(self, legend, orig_handle, fontsize, handlebox):\n",
    "        print(orig_handle)\n",
    "        x0, y0 = handlebox.xdescent, handlebox.ydescent\n",
    "        width, height = handlebox.width, handlebox.height\n",
    "        patch = mpl_text.Text(x=0, y=0, text=orig_handle.my_text, color=orig_handle.my_color, verticalalignment=u'baseline', \n",
    "                                horizontalalignment=u'left', multialignment=None, \n",
    "                                fontproperties=None, rotation=45, linespacing=None, \n",
    "                                rotation_mode=None)\n",
    "        handlebox.add_artist(patch)\n",
    "        return patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def delete_bad_traces_FromList(Traces_List, idx_to_eliminate):\n",
    "    for l in np.arange(len(idx_to_eliminate)):\n",
    "        for i in np.arange(len(Traces_List)):\n",
    "            for j in np.arange(len(Traces_List[i])):\n",
    "                if len(Traces_List[i][j]) == 200:\n",
    "                    if i == idx_to_eliminate[l][0] and j == idx_to_eliminate[l][1]:\n",
    "                        Traces_List[i][j] = [] \n",
    "    return Traces_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def check_trace(trace):\n",
    "#     if np.nanmean(trace) <-50:\n",
    "#         trace = trace + 360\n",
    "# #    if np.nanmean(trace) >400:\n",
    "# #        trace = trace - 360\n",
    "#     return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def check_trace_within_0_to_150(trace):\n",
    "#     if np.nanmean(trace) <-50:\n",
    "#         trace = trace + 360\n",
    "#         if np.nanmean(trace) <-50:\n",
    "#             trace = trace + 360\n",
    "# #    elif np.nanmean(trace[0:60]) >150:\n",
    "# #        trace = []\n",
    "#     else:\n",
    "#         trace = trace\n",
    "#     return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(RidgeX_ExcelList_to_open), len(TA_ExcelList_to_open), len(Centroid_ExcelList_to_open), \\\n",
    "#       len(BodyAxis_ExcelList_to_open))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan(arr):\n",
    "#     mask = np.isnan(arr)\n",
    "#     idx = np.where(~mask,np.arange(mask.size),0)\n",
    "#     np.maximum.accumulate(idx, out=idx)\n",
    "#     arr[mask] = arr[idx]\n",
    "    df = pd.DataFrame(data=arr.flatten())\n",
    "    df = df.fillna(value=None, method='backfill', axis=None, limit=70, downcast=None)\n",
    "    arr = df.values\n",
    "#    print(type(arr))\n",
    "    return arr.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Organize all data into python dict\n",
    "from collections import defaultdict\n",
    "\n",
    "search_key_path = ['*4mm*', '*_5mm*', '*8mm*', '*10mm*']#['*']\n",
    "search_key = ['4mm', '5mm', '8mm', '10mm']\n",
    "\n",
    "dict_ridge_widths = defaultdict(dict)\n",
    "dict_ridge_all = defaultdict(dict)\n",
    "for j in np.arange(len(search_key_path)):\n",
    "    data_location = \"R:/UusisaariU/PROCESSED_DATA_BACKUPS/nRIM_MEMBERS/Salvo/Ridge_Dec2020_March2021_preprocessed/Ridge_Dec2020\"#\"E:/Ridge_Dec2020/\"\n",
    "    RidgeX_ExcelList_to_open = glob.glob(os.path.join(os.path.sep, data_location, search_key_path[j], 'Ridge_X/*.csv'))\n",
    "    TA_ExcelList_to_open = glob.glob(os.path.join(os.path.sep, data_location, search_key_path[j], 'side_cam/*/*.csv'))\n",
    "    Centroid_ExcelList_to_open = glob.glob(os.path.join(os.path.sep, data_location, search_key_path[j], 'Centroid_XY/*.csv'))\n",
    "    BodyAxis_ExcelList_to_open = glob.glob(os.path.join(os.path.sep, data_location, search_key_path[j], 'BodyAxis/*.csv'))\n",
    "#    fig, ax1 = plt.subplots(1, 1, figsize=(15,6))\n",
    "#    print(RidgeX_ExcelList_to_open)\n",
    "#    dict_ridge = {}\n",
    "    for i in np.arange(len(RidgeX_ExcelList_to_open)): # len(peaks)\n",
    "        fig, ax1 = plt.subplots(1, 1, figsize=(15,6))\n",
    "        #Extract arrays\n",
    "        RidgeX_traj = RidgeX_excel_to_array_preprocessed(RidgeX_ExcelList_to_open, chunk_width, i)\n",
    "        BodyAxis_traj = RidgeX_excel_to_array_preprocessed(BodyAxis_ExcelList_to_open, chunk_width, i)\n",
    "        CentroidX_traj, CentroidY_traj = extract_Centroid(Centroid_ExcelList_to_open, chunk_width, i)\n",
    "\n",
    "        #Extract traces of Centroid and Tail Angle around the time frame when the mouse is at the ridge center\n",
    "        a = firstNonNan(CentroidX_traj)\n",
    "        b = round((np.size(CentroidX_traj) - np.count_nonzero(np.isnan(CentroidX_traj)))/2)\n",
    "        c = a + b\n",
    "\n",
    "        #Take tail angle traj after extracting chunk of traj of interest around c\n",
    "        TailAngle_traj = plot_TailAngle(TA_ExcelList_to_open, chunk_width, i, c)\n",
    "        HipAngle_traj = plot_HipAngle(TA_ExcelList_to_open, chunk_width, i, c)\n",
    "        pawangle, RPaw_traj, LPaw_traj = plot_PawAngle(TA_ExcelList_to_open, chunk_width, i)\n",
    "        \n",
    "        RidgeX_traj_chunk = fill_nan(RidgeX_traj[c-chunk_width:c+chunk_width])\n",
    "        TailAngle_traj_chunk = fill_nan(TailAngle_traj[c-chunk_width:c+chunk_width])\n",
    "        HipAngle_traj_chunk = fill_nan(HipAngle_traj[c-chunk_width:c+chunk_width])\n",
    "        RPaw_traj_traj_chunk = RPaw_traj[c-chunk_width:c+chunk_width]\n",
    "        LPaw_traj_traj_chunk = LPaw_traj[c-chunk_width:c+chunk_width]\n",
    "        BodyAxis_traj_chunk = fill_nan(BodyAxis_traj[c-chunk_width:c+chunk_width])\n",
    "\n",
    "        #TailAngle_traj_chunk_corrected = check_trace(TailAngle_traj_chunk)\n",
    "        CentroidX_traj_chunk = fill_nan(CentroidX_traj[c-chunk_width:c+chunk_width])\n",
    "        #print(CentroidX_traj_chunk.ravel())\n",
    "        CentroidY_traj_chunk = fill_nan(CentroidY_traj[c-chunk_width:c+chunk_width])\n",
    "        CentroidX_chunk  = fill_nan(CentroidX_traj[c-chunk_width:c+chunk_width])\n",
    "        CentroidX_chunk_withoutNaN = fill_nan(CentroidX_traj[~np.isnan(CentroidX_traj)]) #drop NaN\n",
    "\n",
    "        #Decide here what variables to plot in the three figures\n",
    "        var1 = np.array(RidgeX_traj_chunk)\n",
    "        var2 = np.array(TailAngle_traj_chunk)\n",
    "        var3 = np.array(CentroidX_traj_chunk)\n",
    "        var4 = np.array([RPaw_traj_traj_chunk, LPaw_traj_traj_chunk, HipAngle_traj_chunk, BodyAxis_traj_chunk, \\\n",
    "                        CentroidY_traj_chunk, CentroidX_traj_chunk])\n",
    "        var5 = CentroidX_traj_chunk\n",
    "        #Plot traces\n",
    "        x1 = np.arange(len(var1))\n",
    "        x2 = np.arange(len(var5))\n",
    "        x3 = np.arange(len(var3))\n",
    "        ax1.plot(x1, var1, color = 'red')\n",
    "        ax1.plot(x2, var5, color = 'black')\n",
    "#        ax1.plot(np.arange(len(BodyAxis_traj_chunk)), BodyAxis_traj_chunk, color = 'green')\n",
    "#        ax1.plot(np.arange(len(LPaw_traj_traj_chunk)), LPaw_traj_traj_chunk, color = 'magenta')\n",
    "        key_file_name = os.path.basename(RidgeX_ExcelList_to_open[i])\n",
    "        ax1.set_title(key_file_name)\n",
    "\n",
    "#         ax1.set_xlabel('Frames (300 Hz)')\n",
    "#         ax1.set_ylabel('Tail Angle Velocity (degrees)')\n",
    "        #Make dict\n",
    "        dict_ridge_widths[search_key[j]][key_file_name] = [var1, var2, var3, var4]\n",
    "\n",
    "        dict_ridge_all['all'][key_file_name] = [var1, var2, var3, var4]\n",
    "        #print(key_file_name)\n",
    "#        plt.title(\"{i}\".format(i = key_file_name))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSTHwrtRidgePeak_beg = 190\n",
    "PSTHwrtRidgePeak_end = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def assign_dict_value_ridge_pos(dict_ridge):\n",
    "    #Divide trials based on ridge position. Assign -1 for left tilt, +1 for right and 0 for no tilts. Append to 4th col\n",
    "    #Changed the threshold from 5000 to 10000 bcs M53 detected many no pert trials as pert\n",
    "    key_list = list(dict_ridge.keys())\n",
    "\n",
    "    for i in np.arange(len(key_list)):\n",
    "\n",
    "        ridge_array = dict_ridge[key_list[i]][0]\n",
    "        ridge_array_translated_nonNaN = ridge_array[~np.isnan(ridge_array)]\n",
    "        ridge_array_translated_nonNaN_mean_centered = ridge_array_translated_nonNaN-  \\\n",
    "        np.nanmean(smooth(ridge_array_translated_nonNaN[20:40]))\n",
    "        ridge_array_translated_int = np.trapz(smooth(ridge_array_translated_nonNaN_mean_centered, 50))\n",
    "        ridge_array_translated_nonNaN_mean_centered_diff_max = max(np.diff(ridge_array_translated_nonNaN_mean_centered))\n",
    "        if ridge_array_translated_int < -3000 and ridge_array_translated_nonNaN_mean_centered_diff_max<40:\n",
    "            dict_ridge[key_list[i]].append(-1)\n",
    "            #print(ridge_array_translated_nonNaN_mean_centered_diff_max)\n",
    "        elif ridge_array_translated_int > +3000 and ridge_array_translated_nonNaN_mean_centered_diff_max<40:\n",
    "            dict_ridge[key_list[i]].append(1)\n",
    "    #        print(ridge_array_translated_int)\n",
    "\n",
    "        else:\n",
    "            dict_ridge[key_list[i]].append(0)    \n",
    "    \n",
    "    return dict_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_dict_value_FlippedRidgeTraces(dict_ridge):\n",
    "    ###Flip ridge traces (1st col) if they have left tilt (5th col, -1 value) so to make easier peak detection.\n",
    "    # Assign to column 6th\n",
    "    values_list = list(dict_ridge.values())\n",
    "    key_list = list(dict_ridge.keys())\n",
    "\n",
    "    for i in np.arange(len(key_list)):\n",
    "        RidgeTraj_classvalue = values_list[i][-1]\n",
    "        RidgeTraj = values_list[i][0]\n",
    "        if RidgeTraj_classvalue == -1:\n",
    "            dict_ridge[key_list[i]].append(-RidgeTraj+250)      \n",
    "        else:\n",
    "            dict_ridge[key_list[i]].append(RidgeTraj)    \n",
    "    return dict_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_dict_value_RidgePeak(dict_ridge):\n",
    "    #Find ridge peak for pert trials (-1, 1 values) and append to 7th column\n",
    "    values_list = list(dict_ridge.values())\n",
    "    key_list = list(dict_ridge.keys())\n",
    "\n",
    "    for i in np.arange(len(key_list)):\n",
    "        RidgeTraj_flipped = values_list[i][-1]\n",
    "        RidgeTraj_classvalue = values_list[i][-2]\n",
    "        if RidgeTraj_classvalue == 1 or RidgeTraj_classvalue == -1:\n",
    "            ridge_array_nonNaN = RidgeTraj_flipped[~np.isnan(RidgeTraj_flipped)] #it's important to remove NaN for findpeaks function\n",
    "            ridge_tilt_peak_idx, _ = find_peaks(ridge_array_nonNaN, height=90, distance = 2000)\n",
    "            dict_ridge[key_list[i]].append(ridge_tilt_peak_idx)\n",
    "\n",
    "\n",
    "        else:\n",
    "            dict_ridge[key_list[i]].append('No Pert')\n",
    "    return dict_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_dict_value_TAClassifier(dict_ridge):\n",
    "    #Take TA value (2nd col) in pert trials (5th col) before ridge peak value (7th col)\n",
    "    #assign -1 for tail on left, +1 for tail on the right and 0 for tail up to 8th col\n",
    "\n",
    "    values_list = list(dict_ridge.values())\n",
    "    key_list = list(dict_ridge.keys())\n",
    "\n",
    "    for i in np.arange(len(key_list)):\n",
    "        RidgeTraj_classvalue = values_list[i][-3]\n",
    "#         fig, ax1 = plt.subplots(1, 1, figsize=(15,6))\n",
    "        if RidgeTraj_classvalue == 1 or RidgeTraj_classvalue == -1:\n",
    "            TailTraj = values_list[i][1]\n",
    "            RidgePeak = values_list[i][-1]\n",
    "            \n",
    "#             plt.plot(values_list[i][1])\n",
    "#             plt.plot(values_list[i][0])\n",
    "#             plt.title(key_list[i])\n",
    "            AvgTailPos_before_tilt = np.nanmean(TailTraj[RidgePeak[0]-30:RidgePeak[0]-20])\n",
    "            if AvgTailPos_before_tilt>190 and AvgTailPos_before_tilt <400:\n",
    "                dict_ridge[key_list[i]].append(-1)\n",
    "            elif AvgTailPos_before_tilt<170 and AvgTailPos_before_tilt > -10:\n",
    "                dict_ridge[key_list[i]].append(+1)  \n",
    "            else:\n",
    "                dict_ridge[key_list[i]].append(0)\n",
    "\n",
    "        else:\n",
    "            dict_ridge[key_list[i]].append('No tail position')\n",
    "    return dict_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_dict_value_TrialClassifier(dict_ridge):\n",
    "    #Take TA classifier (8th col) multiply with Ridge classifier (5th col) and assign result to 9th col\n",
    "    #If value is -1 is controlateral if +1 ipsilateral trial. 0 is undetermined or no pert trials\n",
    "    values_list = list(dict_ridge.values())\n",
    "    key_list = list(dict_ridge.keys())\n",
    "\n",
    "    for i in np.arange(len(key_list)):\n",
    "        Ridge_classvalue = values_list[i][-4]\n",
    "        TA_classvalue = values_list[i][-1]\n",
    "        dict_ridge[key_list[i]].append(Ridge_classvalue*TA_classvalue)   \n",
    "    return dict_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransposeNegBodyAxis(BodyAxis):\n",
    "    if np.nanmean(BodyAxis[chunk_width-50:chunk_width+50])<0:\n",
    "        BodyAxisT = -BodyAxis\n",
    "    else:\n",
    "        BodyAxisT = BodyAxis        \n",
    "    return BodyAxisT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipHA_LPert(HA, RidgeTraj_classvalue):\n",
    "    if RidgeTraj_classvalue == -1:\n",
    "        HAT = (-HA)+270+270 \n",
    "    else:\n",
    "        HAT = HA\n",
    "    return HAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipBodyAxis_LPert(BodyAxisT, RidgeTraj_classvalue):\n",
    "    if RidgeTraj_classvalue == -1:\n",
    "        BodyAxis_T_F = (-BodyAxisT)+180\n",
    "    else:\n",
    "        BodyAxis_T_F = BodyAxisT\n",
    "    return BodyAxis_T_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BodyAxis_distanceto90(BodyAxisT_F):\n",
    "    BodyAxisdist = abs(BodyAxisT_F-90)\n",
    "    return BodyAxisdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_dict_value_FlippedTATraces(dict_ridge):\n",
    "    #Flip TA traces (2nd col) of trials with tail on the left at time of perturbation (8th col) so that in PSTH \n",
    "    #they appear in same direction. Append the new traces in col 10\n",
    "    #Also Flip Steps traces\n",
    "\n",
    "    values_list = list(dict_ridge.values())\n",
    "    key_list = list(dict_ridge.keys())\n",
    "\n",
    "    for i in np.arange(len(key_list)):\n",
    "        TA_classvalue = values_list[i][-2]\n",
    "        TA_trace = values_list[i][1]\n",
    "        Rstep = values_list[i][3][0]\n",
    "        Lstep = values_list[i][3][1]\n",
    "        #Transform body axis by transposing neg traces and flip L pert traces\n",
    "        RidgeTraj_classvalue = values_list[i][-5] #Ridge L, R or no tilt\n",
    "        HA_trace = values_list[i][3][2]\n",
    "        HAT_trace = flipHA_LPert(HA_trace, RidgeTraj_classvalue)\n",
    "        BodyAxis = values_list[i][3][3]\n",
    "        BodyAxisT = TransposeNegBodyAxis(BodyAxis)\n",
    "        BodyAxisT_F = flipBodyAxis_LPert(BodyAxisT, RidgeTraj_classvalue)\n",
    "        BodyAxisdist = BodyAxis_distanceto90(BodyAxisT_F)\n",
    "        CentroidY = values_list[i][3][4]\n",
    "        CentroidX = values_list[i][3][5]\n",
    "#         fig = plt.figure()\n",
    "#         plt.plot(TA_trace)\n",
    "#         plt.plot(BodyAxisT_F)\n",
    "        if TA_classvalue == -1:\n",
    "            TA_trace = -(TA_trace-360)\n",
    "            Centroid_trace = -(CentroidX-150)\n",
    "            dict_ridge[key_list[i]].append([TA_trace, -Lstep, -Rstep, BodyAxisdist, HAT_trace, CentroidY, Centroid_trace]) #Flip L and R step and trace\n",
    "        else:\n",
    "            TA_trace = TA_trace\n",
    "            Centroid_trace = CentroidX-125\n",
    "            dict_ridge[key_list[i]].append([TA_trace, Rstep, Lstep, BodyAxisdist, HAT_trace, CentroidY, Centroid_trace])      \n",
    "    return dict_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_dict_value_TA_and_Ridge_chunkPSTH(dict_ridge):\n",
    "    # Take pert trials (9th col) and based on peak (7th col) extract chunk of flipped Ridge and TA traj (6th and 10th col) \n",
    "    # Assign those traces to col 11 and 12\n",
    "\n",
    "    values_list = list(dict_ridge.values())\n",
    "    key_list = list(dict_ridge.keys())\n",
    "    beg_inter_PSTH = 100\n",
    "    end_inter_PSTH = 150\n",
    "    for i in np.arange(len(key_list)):\n",
    "        PertTrial_classifier = values_list[i][-2]\n",
    "        Ridge_peak = values_list[i][-4]\n",
    "        Ridge_flipped_trace = values_list[i][-5]\n",
    "        TA_flipped_trace = values_list[i][-1][0]\n",
    "        Rstep = values_list[i][-1][1]\n",
    "        Lstep = values_list[i][-1][2]\n",
    "        BodyAxis_trace = values_list[i][-1][3]\n",
    "        HA_trace = values_list[i][-1][4]\n",
    "        CentroidY = values_list[i][-1][5]\n",
    "        CentroidX = values_list[i][-1][6]\n",
    "\n",
    "\n",
    "        if PertTrial_classifier == 1 or PertTrial_classifier == -1:\n",
    "            Ridge_peak = Ridge_peak[0]\n",
    "            Ridge_chunk = Ridge_flipped_trace[Ridge_peak-beg_inter_PSTH:Ridge_peak+end_inter_PSTH]\n",
    "            TA_chunk = TA_flipped_trace[Ridge_peak-beg_inter_PSTH:Ridge_peak+end_inter_PSTH]\n",
    "            HA_chunk = HA_trace[Ridge_peak-beg_inter_PSTH:Ridge_peak+end_inter_PSTH]\n",
    "            RStep_chunk = Rstep[Ridge_peak-beg_inter_PSTH:Ridge_peak+end_inter_PSTH]\n",
    "            LStep_chunk = Lstep[Ridge_peak-beg_inter_PSTH:Ridge_peak+end_inter_PSTH]\n",
    "            BodyAxis_chunk = BodyAxis_trace[Ridge_peak-beg_inter_PSTH:Ridge_peak+end_inter_PSTH]\n",
    "            CentroidY_chunk = CentroidY[Ridge_peak-beg_inter_PSTH:Ridge_peak+end_inter_PSTH]\n",
    "            Ridge_chunk = Ridge_flipped_trace[Ridge_peak-beg_inter_PSTH:Ridge_peak+end_inter_PSTH]\n",
    "            CentroidX_chunk = CentroidX[Ridge_peak-beg_inter_PSTH:Ridge_peak+end_inter_PSTH]\n",
    "            dict_ridge[key_list[i]].append(Ridge_chunk)  \n",
    "            dict_ridge[key_list[i]].append([TA_chunk, RStep_chunk, LStep_chunk, BodyAxis_chunk, HA_chunk, \\\n",
    "                                           CentroidY_chunk, Ridge_chunk, CentroidX_chunk]) \n",
    "\n",
    "        else:\n",
    "            dict_ridge[key_list[i]].append(Ridge_flipped_trace)  \n",
    "            dict_ridge[key_list[i]].append([TA_flipped_trace, Rstep, Lstep, BodyAxis_trace, HA_trace, \\\n",
    "                                           CentroidY, Ridge_flipped_trace, CentroidX])  \n",
    "\n",
    "\n",
    "    return dict_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flipCentroidX_contratrials(CentroidX, PertTrial_classifier):\n",
    "    if PertTrial_classifier == 1:\n",
    "        CentroidX = -CentroidX\n",
    "    elif PertTrial_classifier == -1:\n",
    "        CentroidX = CentroidX\n",
    "    else:\n",
    "        CentroidX = CentroidX\n",
    "    return CentroidX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_highest_peak(peak_idx, trace):\n",
    "    peak_values = trace[peak_idx]\n",
    "    list_peak_values = peak_values.tolist()\n",
    "#    print(list_peak_values)\n",
    "    max_value = max(list_peak_values)\n",
    "    max_idx = list_peak_values.index(max_value)\n",
    "    \n",
    "    return peak_idx[max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_idx_trace_below_thresh(trace, thresh, ridge_peak):\n",
    "    #Take trace leftward to the peak\n",
    "    #trace_left = trace[0:100]\n",
    "    new_idx = 0\n",
    "    i = ridge_peak\n",
    "    trace = np.diff(smooth(trace, 20))\n",
    "    #print(trace, thresh, ridge_peak)\n",
    "    while i > 0:\n",
    "        if trace[i] < thresh:\n",
    "            new_idx = i\n",
    "            break\n",
    "        else:\n",
    "            i -= 1\n",
    "    return new_idx-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignSideandTopCamTraces(dict_ridge):\n",
    "    #Take ridgex and Centroid X traces and find peaks and delay between those two, and center PSTH of two traces\n",
    "    values_list = list(dict_ridge.values())\n",
    "    key_list = list(dict_ridge.keys())\n",
    "    beg_inter_PSTH = 100\n",
    "    end_inter_PSTH = 150\n",
    "    for i in np.arange(len(key_list)):\n",
    "        PertTrial_classifier = values_list[i][-4]\n",
    "        Ridge_flipped_trace = values_list[i][-7]\n",
    "        TA_flipped_trace = values_list[i][-3][0]\n",
    "        Rstep = values_list[i][-3][1]\n",
    "        Lstep = values_list[i][-3][2]\n",
    "        BodyAxis_trace = values_list[i][-3][3]\n",
    "        HA_trace = values_list[i][-3][4]\n",
    "        CentroidY = values_list[i][-3][5]\n",
    "\n",
    "\n",
    "        #Find peaks CentroidX\n",
    "        CentroidX = values_list[i][-3][6]\n",
    "        CentroidX_flipped = flipCentroidX_contratrials(CentroidX, PertTrial_classifier)\n",
    "        CentroidX_nonNaN = CentroidX_flipped[~np.isnan(CentroidX_flipped)]\n",
    "        Centroid_X_diff_smoothed = np.diff(smooth(CentroidX_nonNaN, 20))\n",
    "        CentroidX_peak_idx, _ = find_peaks(Centroid_X_diff_smoothed, prominence = 0.2)\n",
    "\n",
    "        #Find peaks RidgeX\n",
    "        Ridge_nonNan = Ridge_flipped_trace[~np.isnan(Ridge_flipped_trace)]\n",
    "        Ridge_X_diff_smoothed = np.diff(smooth(Ridge_nonNan, 20))\n",
    "        RidgeX_peak_idx, _ = find_peaks(Ridge_X_diff_smoothed, prominence = 0.2)\n",
    "    \n",
    "        #Use ridge_peak for centering sidecam traces and CentroidX_peak_idx for topcam traces    \n",
    "        if (PertTrial_classifier == 1 or PertTrial_classifier == -1) and len(CentroidX_peak_idx) \\\n",
    "        and len(Ridge_flipped_trace):\n",
    "            #Find peak begin in RidgeX trace and use that as index for side cam traces\n",
    "            RidgeXHighestPeak = choose_highest_peak(RidgeX_peak_idx, Ridge_X_diff_smoothed)\n",
    "            Ridge_peak = RidgeXHighestPeak\n",
    "            pert_beg_idx = find_idx_trace_below_thresh(Ridge_flipped_trace, 0.5, Ridge_peak)\n",
    "            Ridge_chunk = Ridge_flipped_trace[pert_beg_idx-beg_inter_PSTH:pert_beg_idx+end_inter_PSTH]\n",
    "            TA_chunk = TA_flipped_trace[pert_beg_idx-beg_inter_PSTH:pert_beg_idx+end_inter_PSTH]\n",
    "            RStep_chunk = Rstep[pert_beg_idx-beg_inter_PSTH:pert_beg_idx+end_inter_PSTH]\n",
    "            LStep_chunk = Lstep[pert_beg_idx-beg_inter_PSTH:pert_beg_idx+end_inter_PSTH]\n",
    "            HA_chunk = HA_trace[pert_beg_idx-beg_inter_PSTH:pert_beg_idx+end_inter_PSTH]\n",
    "            #Find peak begin in CentroidX trace and use that as index for top cam traces\n",
    "            CentroidXHighestPeak = choose_highest_peak(CentroidX_peak_idx, Centroid_X_diff_smoothed)\n",
    "            CentroidX_peak_idx = CentroidXHighestPeak\n",
    "            pert_beg_idx_topcam = find_idx_trace_below_thresh(CentroidX_flipped, 0.5, CentroidX_peak_idx)\n",
    "            CentroidX_chunk = CentroidX[pert_beg_idx_topcam-beg_inter_PSTH:pert_beg_idx_topcam+end_inter_PSTH]\n",
    "            CentroidY_chunk = CentroidY[pert_beg_idx_topcam-beg_inter_PSTH:pert_beg_idx_topcam+end_inter_PSTH]\n",
    "            BodyAxis_chunk = BodyAxis_trace[pert_beg_idx_topcam-beg_inter_PSTH:pert_beg_idx_topcam+end_inter_PSTH]\n",
    "            \n",
    "            dict_ridge[key_list[i]].append(Ridge_chunk) \n",
    "            dict_ridge[key_list[i]].append([TA_chunk, RStep_chunk, LStep_chunk, BodyAxis_chunk, HA_chunk, \\\n",
    "                                           CentroidY_chunk, Ridge_chunk, CentroidX_chunk])\n",
    "# #            fig, ax1 = plt.subplots(1, 1, figsize=(15,6))\n",
    "#             plt.plot(CentroidX_chunk)#, CentroidX_chunk)\n",
    "#             plt.plot(Ridge_chunk)#, CentroidX_chunk)\n",
    "\n",
    "# #             plt.plot(TA_chunk)\n",
    "            #if len(CentroidX_chunk):\n",
    "                #plt.plot(CentroidX_chunk, 'y')\n",
    "#             #plt.plot(np.diff(smooth(Ridge_chunk, 20))*50, 'r')#, CentroidX_chunk)\n",
    "# #             plt.plot(CentroidX_peak_idx, Centroid_X_diff_smoothed[CentroidX_peak_idx], 'x')\n",
    "# #             plt.title(CentroidX_peak_idx)\n",
    "\n",
    "        else:\n",
    "            dict_ridge[key_list[i]].append(Ridge_flipped_trace)  \n",
    "            dict_ridge[key_list[i]].append([TA_flipped_trace, Rstep, Lstep, BodyAxis_trace, HA_trace, \\\n",
    "                                           CentroidY, Ridge_flipped_trace, CentroidX])  \n",
    "\n",
    "\n",
    "    return dict_ridge    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_traces_after_QC(dict_ridge):\n",
    "    #exclude traces where ridge trajectory does not look like perturbation trial\n",
    "    list_filenameToExclude = ['M53_Pert_4mm-12072020170241-0000.csv', 'M49_Pert_5mm-12142020110359-0000.csv', 'M50_Pert_8mm-12142020122848-0000.csv',#contra trials\n",
    "                              'M54_Pert_4mm-12072020170519-0000.csv', 'M48_Pert_4mm-12042020111831-0000.csv', 'M56_Pert_8mm-12042020135237-0000.csv',\n",
    "                              'M51_RPert_4mm-12032020112338-0000.csv', 'M50_Pert_4mm-12042020101633-0000.csv', 'M56_Pert_8mm-12042020142051-0000.csv',\n",
    "                              'M54_Pert_4mm-12072020170621-0000.csv', 'M51_Pert_4mm-12042020104950-0000.csv', 'M56_Pert_8mm-12042020142126-0000.csv',\n",
    "                              'M56_Pert_4mm-12072020171624-0000.csv', 'M51_Pert_4mm-12042020105106-0000.csv', 'M53_Pert_10mm-12062020152419-0000.csv', \n",
    "                              'M57_Pert_10mm-12062020160614-0000.csv', 'M59_Pert_10mm-12152020182329-0000.csv', 'M60_Pert_10mm-12152020174839-0000.csv', \n",
    "                              'M62_Pert_10mm-12152020184206-0000.csv',\n",
    "                              'M57_Pert_4mm-12072020172105-0000.csv', 'M52_Pert_4mm-12042020102648-0000.csv',\n",
    "                              'M57_Pert_4mm-12072020172240-0000.csv', 'M59_Pert_5mm-12062020180133-0000.csv',\n",
    "                              'M58_Pert_4mm-12172020171736-0000.csv', 'M60_Pert_5mm-12062020181101-0000.csv',\n",
    "                              'M58_Pert_4mm-12172020172239-0000.csv', 'M61_Pert_5mm-12062020181805-0000.csv',\n",
    "                              'M58_Pert_4mm-12172020172348-0000.csv', 'M62_Pert_5mm-12062020182330-0000.csv',\n",
    "                              'M59_Pert_4mm-12172020172433-0000.csv',\n",
    "                              'M59_Pert_4mm-12172020174857-0000.csv',\n",
    "                              'M61_Pert_4mm-12172020175548-0000.csv',\n",
    "                              'M48_Pert_4mm-12132020125937-0000.csv', 'M52_Pert_5mm-12142020114124-0000.csv', #ipsi\n",
    "                              'M53_Pert_8mm-12142020145729-0000.csv', 'M53_Pert_8mm-12042020143250-0000.csv',\n",
    "                              'M49a_Pert_10mm-12152020155834-0000.csv', 'M49a_Pert_10mm-12152020155945-0000.csv', \n",
    "                              'M49a_Pert_10mm-12152020160050-0000.csv', 'M53_Pert_10mm-12062020154548-0000.csv', \n",
    "                              'M55_Pert_10mm-12062020153001-0000.csv', 'M58_Pert_10mm-12152020172956-0000.csv',\n",
    "                              'M54_Pert_10mm-12062020154831-0000.csv', 'M61_Pert_10mm-12152020175056-0000.csv', \n",
    "                              'M58_Pert_10mm-12042020163408-0000.csv', 'M58_Pert_10mm-12042020163610-0000.csv', \n",
    "                              'M58_Pert_10mm-12042020163743-0000.csv', 'M59_Pert_10mm-12042020182350-0000.csv',\n",
    "                              'M61_Pert_10mm-12042020165302-0000.csv',  \n",
    "                              'M53_Pert_8mm-12042020143320-0000.csv', 'M56_Pert_8mm-12042020142337-0000.csv', \n",
    "                              'M57_Pert_8mm-12042020135806-0000.csv', 'M57_Pert_8mm-12042020142413-0000.csv',\n",
    "                              'M57_Pert_8mm-12042020142609-0000.csv', 'M57_Pert_8mm-12042020142639-0000.csv', \n",
    "                              'M57_Pert_8mm-12042020142738-0000.csv', 'M57_Pert_8mm-12042020142812-0000.csv', \n",
    "                              'M60_Pert_8mm-12052020183057-0000.csv',\n",
    "                              'M50_Pert_4mm-12132020121048-0000.csv', 'M48_aPert_4mm-12042020100650-0000.csv',\n",
    "                              'M50_Pert_4mm-12132020121349-0000.csv', 'M50_Pert_4mm-12042020104554-0000.csv',\n",
    "                              'M50_Pert_4mm-12132020121541-0000.csv', 'M50_Pert_4mm-12042020104655-0000.csv',\n",
    "                              'M50_Pert_4mm-12132020121048-0000.csv', 'M52_Pert_4mm-12042020102432-0000.csv',\n",
    "                              'M57_Pert_4mm-12172020153455-0000.csv', 'M58_Pert_5mm-12062020175346-0000.csv',\n",
    "                              'M56_Pert_4mm-12072020171819-0000.csv', 'M60_Pert_5mm-12062020181412-0000.csv',\n",
    "                              'M60_Pert_4mm-12172020175131-0000.csv', 'M58_Pert_4mm-12172020174405-0000.csv',\n",
    "                              'M58_Pert_10mm-12042020163639-0000.csv', 'M58_Pert_10mm-12152020182235-0000.csv']\n",
    "\n",
    "    for i in np.arange(len(list_filenameToExclude)):\n",
    "        key_to_be_deleted = list_filenameToExclude[i]\n",
    "        dict_ridge.pop(key_to_be_deleted, None)\n",
    "    return dict_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excludeNaNTATraces(dict_ridge):\n",
    "    #Exclude from dict all trials where the TA traj is mostly NaN\n",
    "    values_list = list(dict_ridge.values())\n",
    "    key_list = list(dict_ridge.keys())\n",
    "    a = 0\n",
    "    for i in np.arange(len(key_list)):\n",
    "        Trial_classvalue = values_list[i][-5]\n",
    "        TA_traj = values_list[i][-1][0]#[0:250]\n",
    "        BodyAxis_trace = values_list[i][-1][3]\n",
    "        CentroidX_trace = values_list[i][-1][7]\n",
    "        no_of_nan_TAtraj = list(np.isnan(TA_traj))\n",
    "        count_NaN = no_of_nan_TAtraj.count(1)\n",
    "# #        fig = plt.figure()\n",
    "#         if Trial_classvalue == 1:\n",
    "#             #plt.plot(TA_traj, 'b')\n",
    "#             a = a+1\n",
    "#             print(a)\n",
    "#             plt.plot(BodyAxis_trace, 'r')\n",
    "        if count_NaN>100:\n",
    "            key_to_be_deleted = key_list[i]\n",
    "            dict_ridge.pop(key_to_be_deleted, None)\n",
    "        elif len(TA_traj) == 0:\n",
    "            key_to_be_deleted = key_list[i]\n",
    "            dict_ridge.pop(key_to_be_deleted, None)    \n",
    "        elif len(CentroidX_trace) == 0:\n",
    "            key_to_be_deleted = key_list[i]\n",
    "            dict_ridge.pop(key_to_be_deleted, None)    \n",
    "    return dict_ridge      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function to count swings\n",
    "# def count_swing_foronetrial(TA_traj):\n",
    "#     print(TA_traj)\n",
    "#     TA_TiltResp_traj = TA_traj[70:110]\n",
    "#     if any(value >=180 for value in TA_TiltResp_traj):\n",
    "#         value_to_appendDict = 1  \n",
    "#     elif any(value <180 for value in TA_TiltResp_traj):\n",
    "#         value_to_appendDict = 0\n",
    "#     else:\n",
    "#         value_to_appendDict = -3 #issue\n",
    "#     return value_to_appendDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def assign_dict_value_swing_classifier(dict_ridge):\n",
    "#     # Take pert trials (9th col) and count swings\n",
    "#     # Assign 1 for swing, 0 for no swing, -1 for non pert trials. Those classifier are appended to col 13\n",
    "\n",
    "#     values_list = list(dict_ridge.values())\n",
    "#     key_list = list(dict_ridge.keys())\n",
    "\n",
    "#     for i in np.arange(len(key_list)):\n",
    "#         PertTrial_classifier = values_list[i][8]\n",
    "#         TA_traj = values_list[i][11]#[0:250]\n",
    "#         if PertTrial_classifier == -1 or PertTrial_classifier == 1:\n",
    "#             value_to_appendDict = count_swing_foronetrial(TA_traj)\n",
    "#             dict_ridge[key_list[i]].append(value_to_appendDict)          \n",
    "#         else:\n",
    "#             dict_ridge[key_list[i]].append(-3) #issue\n",
    "#     return dict_ridge      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def assign_dict_value_swing_classifier_wrt_TrialType(dict_ridge):\n",
    "#     # Take 13 col and multiply with pert classifier. You will get -1 for all swings in ipsilateral trials \n",
    "#     # -1 for swings in controlateral trials 0 for no swing and NaN for anything else. Append to 14th col\n",
    "\n",
    "#     values_list = list(dict_ridge.values())\n",
    "#     key_list = list(dict_ridge.keys())\n",
    "\n",
    "#     for i in np.arange(len(key_list)):\n",
    "#         Swing_classifier = values_list[i][-1]\n",
    "#         Trial_classifier = values_list[i][8]\n",
    "#         #print(Trial_classifier, Swing_classifier)\n",
    "#         dict_ridge[key_list[i]].append(Trial_classifier*Swing_classifier)\n",
    "#         TA_traj = values_list[i][11][0:250]\n",
    "# #         if Trial_classifier*Swing_classifier == -1:\n",
    "# #             plt.plot(TA_traj)\n",
    "#     return dict_ridge      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Function to get percentages of swings\n",
    "# def percentage_swing_acrosstrials(dictionary, count_ipsi_trials):\n",
    "#     values_list = list(dictionary.values())\n",
    "#     key_list = list(dictionary.keys())\n",
    "#     n = 13 #column in dict with swing classifier wrt trial type (1 swing ipsi, -1 swing contra, 0 no swing)\n",
    "#     dict_swing_resp = {k:v[n] for k,v in dictionary.items()}\n",
    "#     values_list = list(dict_swing_resp.values())\n",
    "#     swing_count = values_list.count(1)\n",
    "#     perc_swing = swing_count/count_ipsi_trials\n",
    "#     return(perc_swing)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_dict_swing_perc(dict_ridge):\n",
    "#     #Write dictionary where key is Mouse ID and value is % of swings in ipsi and contra trials\n",
    "#     search_key = ['M48', 'M49', 'M50', 'M51', 'M52', 'M53', 'M54', 'M55', 'M56', 'M57', 'M58', 'M59', 'M60', 'M61', 'M62']\n",
    "#     dict_percswings = {}\n",
    "#     for i in np.arange(len(search_key)):\n",
    "#         subdict = dict(filter(lambda item: search_key[i] in item[0], dict_ridge.items())) \n",
    "#         values_list = list(subdict.values())\n",
    "#         key_list = list(subdict.keys()) \n",
    "#         n = 13 #column in dict with swing classifier wrt trial type (1 swing ipsi, -1 swing contra, 0 no swing)\n",
    "#         j = 8 #column in dict with trial classifier\n",
    "#         dict_swing_resp = {k:v[n] for k,v in subdict.items()}\n",
    "#         dict_trial_classifier = {k:v[j] for k,v in subdict.items()}\n",
    "#         values_swing_classifier = list(dict_swing_resp.values())\n",
    "#         values_trial_classifier = list(dict_trial_classifier.values())\n",
    "#         swing_count_ipsi = values_swing_classifier.count(1)\n",
    "#         count_trials_ipsi = values_trial_classifier.count(1)\n",
    "#         perc_swing_ipsi = swing_count_ipsi/count_trials_ipsi  \n",
    "#         swing_count_contra = values_swing_classifier.count(-1)\n",
    "#         count_trials_contra = values_trial_classifier.count(-1)\n",
    "#         perc_swing_contra = swing_count_contra/count_trials_contra    \n",
    "#         #print(perc_swing_ipsi, perc_swing_contra)\n",
    "#         #Make dict\n",
    "#         key_name = search_key[i]\n",
    "#         dict_percswings[key_name] = perc_swing_ipsi, perc_swing_contra\n",
    "#         #dict_percswings[key_name].append([perc_swing_ipsi, perc_swing_contra])\n",
    "#     return dict_percswings      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Write script to pre-process and organize all pert trial into python dict for dict with seperate widths\n",
    "\n",
    "search_key = ['4mm', '5mm', '8mm', '10mm']\n",
    "dict_percswings_widths = defaultdict(dict)\n",
    "\n",
    "for i in np.arange(len(search_key)):\n",
    "    dict_ridge = dict_ridge_widths[search_key[i]]\n",
    "    dict_ridge = exclude_traces_after_QC(dict_ridge)\n",
    "    dict_ridge = assign_dict_value_ridge_pos(dict_ridge)\n",
    "    dict_ridge = assign_dict_value_FlippedRidgeTraces(dict_ridge)\n",
    "    dict_ridge = assign_dict_value_RidgePeak(dict_ridge)\n",
    "    dict_ridge = assign_dict_value_TAClassifier(dict_ridge)\n",
    "    dict_ridge = assign_dict_value_TrialClassifier(dict_ridge)\n",
    "    dict_ridge = assign_dict_value_FlippedTATraces(dict_ridge)\n",
    "    dict_ridge = assign_dict_value_TA_and_Ridge_chunkPSTH(dict_ridge)\n",
    "    #dict_ridge = alignSideandTopCamTraces(dict_ridge)\n",
    "    dict_percswings_widths[search_key[i]] = excludeNaNTATraces(dict_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Write script to pre-process and organize all pert trial into python dict\n",
    "\n",
    "search_key = ['all']#, '5mm', '8mm', '10mm']\n",
    "dict_percswings_all = defaultdict(dict)\n",
    "\n",
    "for i in np.arange(len(search_key)):\n",
    "    dict_ridge = dict_ridge_all[search_key[i]]\n",
    "    dict_ridge = exclude_traces_after_QC(dict_ridge)\n",
    "    dict_ridge = assign_dict_value_ridge_pos(dict_ridge)\n",
    "    dict_ridge = assign_dict_value_FlippedRidgeTraces(dict_ridge)\n",
    "    dict_ridge = assign_dict_value_RidgePeak(dict_ridge)\n",
    "    dict_ridge = assign_dict_value_TAClassifier(dict_ridge)\n",
    "    dict_ridge = assign_dict_value_TrialClassifier(dict_ridge)\n",
    "    dict_ridge = assign_dict_value_FlippedTATraces(dict_ridge)\n",
    "    dict_ridge = assign_dict_value_TA_and_Ridge_chunkPSTH(dict_ridge)\n",
    "    #dict_ridge = alignSideandTopCamTraces(dict_ridge)\n",
    "    dict_percswings_all[search_key[i]] = excludeNaNTATraces(dict_ridge)\n",
    "    \n",
    "#     dict_ridge = assign_dict_value_swing_classifier(dict_ridge)\n",
    "#     dict_ridge = assign_dict_value_swing_classifier_wrt_TrialType(dict_ridge)\n",
    "#     dict_percswings = create_dict_swing_perc(dict_ridge)\n",
    "#     dict_percswings_all[search_key[i]] = dict_percswings\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Print out for exporting to prism\n",
    "# dict_percswings_all.values()\n",
    "# search_key = ['all']#, '5mm', '8mm', '10mm']\n",
    "# search_mouse_key = ['M48', 'M49', 'M50', 'M51', 'M52', 'M53', 'M54', 'M55', 'M56', 'M57', 'M58', 'M59', 'M60', 'M61', 'M62']\n",
    "\n",
    "# #dict_percswings = {}\n",
    "# for i in np.arange(len(search_key)):\n",
    "#     print(search_key[i])\n",
    "#     for j in np.arange(len(search_mouse_key)):\n",
    "#         print(dict_percswings_all[search_key[i]][search_mouse_key[j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Good code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrajArray_from_dict(dict):\n",
    "    data = list(dict.items())\n",
    "    an_array = np.array(data)\n",
    "    return an_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_Mean_STD_forPSTH(array_value_dict):\n",
    "    mean_array = np.nanmean(array_value_dict, axis = 0)\n",
    "    STD_array = stats.sem(array_value_dict, nan_policy='omit')\n",
    "    return mean_array, STD_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_quadrant_classifier(TA, no_quad = 6):\n",
    "    lst = np.arange(361)\n",
    "    chunks_list = np.array_split(lst, no_quad)\n",
    "    classifier = []\n",
    "    for i in np.arange(len(chunks_list)):\n",
    "        first_value = chunks_list[i][0]\n",
    "        last_value = chunks_list[i][-1]\n",
    "        if first_value <= TA <= last_value:\n",
    "            classifier = i\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excludeTATracesHighDerivative(dict_ridge):\n",
    "    #Exclude from dict all trials where the TA traj derivative is high\n",
    "    values_list = list(dict_ridge.values())\n",
    "    key_list = list(dict_ridge.keys()) #trial\n",
    "    for i in np.arange(len(key_list)):\n",
    "        TA_traj = smooth(values_list[i][-3][0])\n",
    "        TA_diff = np.diff(TA_traj)\n",
    "        #print(TA_diff)\n",
    "        if np.any(TA_diff>100) or np.any(TA_diff<-100):\n",
    "            key_to_be_deleted = key_list[i]\n",
    "            dict_ridge.pop(key_to_be_deleted, None)\n",
    "\n",
    "    return dict_ridge   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def excludeTATracesabove180(dict_ridge):\n",
    "    #Exclude from dict all trials where the TA traj is higher than 180 before pert\n",
    "    values_list = list(dict_ridge.values())\n",
    "    key_list = list(dict_ridge.keys()) #trial\n",
    "    for i in np.arange(len(key_list)):\n",
    "        TA_traj = values_list[i][-1][0]#[0:250]\n",
    "        TA_traj_before_pert = TA_traj[0:100]\n",
    "        #BodyAxis = values_list[i][-3][3]\n",
    "        #plt.plot(TA_traj_before_pert)\n",
    "        if np.any(TA_traj>600) or np.any(TA_traj<-70):# or np.any(BodyAxis>150) or np.any(BodyAxis<50):\n",
    "            key_to_be_deleted = key_list[i]\n",
    "            dict_ridge.pop(key_to_be_deleted, None)\n",
    "\n",
    "    return dict_ridge   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitsequenceequally(seq, num):\n",
    "    avg = len(seq) / float(num)\n",
    "    out = []\n",
    "    last = 0.0\n",
    "\n",
    "    while last < len(seq):\n",
    "        out.append(seq[int(last):int(last + avg)])\n",
    "        last += avg\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot trials of same width together changing the dict_ridge_all key\n",
    "search_key = ['M48', 'M49', 'M50', 'M51', 'M52', 'M53', 'M54', 'M55', 'M56', 'M57', 'M58', 'M59', 'M60', 'M61', 'M62']\n",
    "dict_ridge_4mm = dict_percswings_all['all']\n",
    "\n",
    "\n",
    "no_quad_to_plot = 200\n",
    "TA_beg_PSTH_list = []\n",
    "TA_end_PSTH_list = []\n",
    "TAVel_beg_PSTH_list = []\n",
    "class_value_list = []\n",
    "Theta_theta_prime_list = []\n",
    "TA_end_end_PSTH_list = []\n",
    "a = 0\n",
    "HA_traj_list = []\n",
    "TA_traj_list = []\n",
    "HA_traj_list_AngMom = []\n",
    "TA_traj_list_AngMom = []\n",
    "Ridge_traj_list = []\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(15,6))\n",
    "for i in np.arange(len(search_key)):\n",
    "    res = dict(filter(lambda item: search_key[i] in item[0], dict_ridge_4mm.items())) \n",
    "#     res = excludeTATracesHighDerivative(res)\n",
    "    res = excludeTATracesabove180(res)\n",
    "    values_filename_list = list(res.values())\n",
    "    key_filename_list = list(res.keys()) \n",
    "    for i in np.arange(len(key_filename_list)):\n",
    "            Trial_classvalue = values_filename_list[i][-4]\n",
    "            #print(Trial_classvalue)\n",
    "            if Trial_classvalue ==1:\n",
    "                RidgePert_classvalue = values_filename_list[i][-10]\n",
    "                HA_traj = (values_filename_list[i][-1][4])\n",
    "                TA_traj = (values_filename_list[i][-1][0])\n",
    "                Ridge_traj = (values_filename_list[i][-1][6])/5\n",
    "                if len(TA_traj) == 250 and len(HA_traj) == 250:# and TA_traj[120]>200:\n",
    "                    plt.plot(TA_traj,'b', lw=2, alpha = 0.1)\n",
    "                    plt.plot(Ridge_traj,'r', lw=2, alpha = 0.1)\n",
    "                    TA_traj_list.append(np.diff(TA_traj))\n",
    "                    HA_traj_list.append(np.diff(HA_traj))\n",
    "                    Ridge_traj_list.append(-np.diff(Ridge_traj))\n",
    "                    HA_AngMom = np.diff(HA_traj*40)\n",
    "                    TA_AngMom = np.diff(TA_traj*8.15)\n",
    "                    TA_traj_list_AngMom.append(np.trapz(TA_AngMom[100:160]))\n",
    "                    HA_traj_list_AngMom.append(np.trapz(HA_AngMom[100:160]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PSTH_Mean_STD_label_color_pre_assigned(mean_array, std_array, ax):\n",
    "    clrs = sns.color_palette(\"husl\", len(mean_array))\n",
    "    label_strings = ['Tail', 'Ridge', 'Hip', 'Tail+Hip']#, 'Ipsi 4mm', 'Contra 4mm', 'Ipsi 4mm', 'Contra 4mm']\n",
    "#    _, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    with sns.axes_style(\"darkgrid\"):\n",
    "        for i in range(len(mean_array)):\n",
    "            mean_array[i] = smooth(mean_array[i], 10)[60:150]\n",
    "            std_array[i] = smooth(std_array[i], 10)[60:150]\n",
    "            x = np.linspace(0, len(mean_array[i])/300, len(mean_array[i]))\n",
    "            ax.plot(x, mean_array[i], c=clrs[i], label = label_strings[i])\n",
    "            ax.fill_between(x, mean_array[i]-std_array[i], mean_array[i]+std_array[i] ,alpha=0.3, facecolor=clrs[i])\n",
    "            ax.legend(loc=\"upper right\", fontsize = 13, frameon=False)\n",
    "            ax.set_xlabel('Frame number (FR = 300 Hz)')\n",
    "            ax.set_ylabel('Angular Momentum (g-cm2/s)')\n",
    "            #ax.axvline(105,0,360, color = 'red')\n",
    "            #ax.axvline(60,0,360, color = 'red')\n",
    "            #ax.axvline(140,0,360, color = 'red')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-61-59c9813e4b4d>:2: RuntimeWarning: Mean of empty slice\n",
      "  mean_array = np.nanmean(array_value_dict, axis = 0)\n",
      "C:\\Users\\Salvo\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:261: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\Salvo\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:221: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\Salvo\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:253: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "smooth only accepts 1 dimension arrays.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-e6e064387f4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mclrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolor_palette\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"husl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mplot_PSTH_Mean_STD_label_color_pre_assigned\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Time (s)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Angular Momentum (g-cm2/s)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m18\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-67-343f6c3675bd>\u001b[0m in \u001b[0;36mplot_PSTH_Mean_STD_label_color_pre_assigned\u001b[1;34m(mean_array, std_array, ax)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes_style\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"darkgrid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mmean_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[0mstd_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstd_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-08b2d83db8f3>\u001b[0m in \u001b[0;36msmooth\u001b[1;34m(x, window_len, window)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"smooth only accepts 1 dimension arrays.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mwindow_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: smooth only accepts 1 dimension arrays."
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+AAAAKZCAYAAAA4fUHAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlkElEQVR4nO3df2zV9b348Veh0Kr3toswKwgy3NXJLhm7lMAot1l0WgOGG5LdwOKNqBeTNdsuAa7egdzoICbN3c3MvU7BLYJmCXobf8Y/eh3Nzb38EG4ymrIsQu4W4VrYWkkxa1F3i8Dn+4df+v32tiintC9AH4/k/HHee79P32d5r+7p55x+yoqiKAIAAAAYVWMu9gYAAADgs0CAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAgpIDfOfOnbF48eKYPHlylJWVxauvvvqJa3bs2BG1tbVRWVkZN9xwQzz11FPD2SsAAABctkoO8Pfffz9mzZoVTzzxxHnNP3z4cCxatCjq6+ujvb09HnrooVi5cmW89NJLJW8WAAAALldlRVEUw15cVhavvPJKLFmy5Jxzvv/978drr70WBw8e7B9rbGyMX/7yl7F3797h/mgAAAC4rJSP9g/Yu3dvNDQ0DBi74447YsuWLfHhhx/GuHHjBq3p6+uLvr6+/udnzpyJd999NyZMmBBlZWWjvWUAAAA+44qiiBMnTsTkyZNjzJiR+fNpox7gXV1dUVNTM2CspqYmTp06Fd3d3TFp0qRBa5qammLDhg2jvTUAAAD4WEeOHIkpU6aMyGuNeoBHxKCr1mc/9X6uq9nr1q2LNWvW9D/v6emJ66+/Po4cORJVVVWjt1EAAACIiN7e3pg6dWr88R//8Yi95qgH+LXXXhtdXV0Dxo4dOxbl5eUxYcKEIddUVFRERUXFoPGqqioBDgAAQJqR/Br0qN8HfP78+dHa2jpgbPv27TFnzpwhv/8NAAAAn0YlB/h7770X+/fvj/3790fER7cZ279/f3R0dETERx8fX758ef/8xsbGePvtt2PNmjVx8ODB2Lp1a2zZsiUeeOCBkXkHAAAAcBko+SPo+/bti1tuuaX/+dnvat9zzz3x7LPPRmdnZ3+MR0RMnz49WlpaYvXq1fHkk0/G5MmT4/HHH49vfvObI7B9AAAAuDxc0H3As/T29kZ1dXX09PT4DjgAAACjbjQ6dNS/Aw4AAAAIcAAAAEghwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEwwrwTZs2xfTp06OysjJqa2tj165dHzt/27ZtMWvWrLjyyitj0qRJcd9998Xx48eHtWEAAAC4HJUc4M3NzbFq1apYv359tLe3R319fSxcuDA6OjqGnL979+5Yvnx5rFixIt5888144YUX4he/+EXcf//9F7x5AAAAuFyUHOCPPfZYrFixIu6///6YMWNG/NM//VNMnTo1Nm/ePOT8//zP/4wvfOELsXLlypg+fXr8+Z//eXz729+Offv2XfDmAQAA4HJRUoCfPHky2traoqGhYcB4Q0ND7NmzZ8g1dXV1cfTo0WhpaYmiKOKdd96JF198Me68885z/py+vr7o7e0d8AAAAIDLWUkB3t3dHadPn46ampoB4zU1NdHV1TXkmrq6uti2bVssW7Ysxo8fH9dee2187nOfix//+Mfn/DlNTU1RXV3d/5g6dWop2wQAAIBLzrD+CFtZWdmA50VRDBo768CBA7Fy5cp4+OGHo62tLV5//fU4fPhwNDY2nvP1161bFz09Pf2PI0eODGebAAAAcMkoL2XyxIkTY+zYsYOudh87dmzQVfGzmpqaYsGCBfHggw9GRMRXvvKVuOqqq6K+vj4effTRmDRp0qA1FRUVUVFRUcrWAAAA4JJW0hXw8ePHR21tbbS2tg4Yb21tjbq6uiHXfPDBBzFmzMAfM3bs2Ij46Mo5AAAAfBaU/BH0NWvWxNNPPx1bt26NgwcPxurVq6Ojo6P/I+Xr1q2L5cuX989fvHhxvPzyy7F58+Y4dOhQvPHGG7Fy5cqYO3duTJ48eeTeCQAAAFzCSvoIekTEsmXL4vjx47Fx48bo7OyMmTNnRktLS0ybNi0iIjo7OwfcE/zee++NEydOxBNPPBF/+7d/G5/73Ofi1ltvjX/4h38YuXcBAAAAl7iy4jL4HHhvb29UV1dHT09PVFVVXeztAAAA8Ck3Gh06rL+CDgAAAJRGgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkGFaAb9q0KaZPnx6VlZVRW1sbu3bt+tj5fX19sX79+pg2bVpUVFTEF7/4xdi6deuwNgwAAACXo/JSFzQ3N8eqVati06ZNsWDBgvjJT34SCxcujAMHDsT1118/5JqlS5fGO++8E1u2bIk/+ZM/iWPHjsWpU6cuePMAAABwuSgriqIoZcG8efNi9uzZsXnz5v6xGTNmxJIlS6KpqWnQ/Ndffz2+9a1vxaFDh+Lqq68e1iZ7e3ujuro6enp6oqqqalivAQAAAOdrNDq0pI+gnzx5Mtra2qKhoWHAeENDQ+zZs2fINa+99lrMmTMnfvjDH8Z1110XN910UzzwwAPxhz/8Yfi7BgAAgMtMSR9B7+7ujtOnT0dNTc2A8Zqamujq6hpyzaFDh2L37t1RWVkZr7zySnR3d8d3vvOdePfdd8/5PfC+vr7o6+vrf97b21vKNgEAAOCSM6w/wlZWVjbgeVEUg8bOOnPmTJSVlcW2bdti7ty5sWjRonjsscfi2WefPedV8Kampqiuru5/TJ06dTjbBAAAgEtGSQE+ceLEGDt27KCr3ceOHRt0VfysSZMmxXXXXRfV1dX9YzNmzIiiKOLo0aNDrlm3bl309PT0P44cOVLKNgEAAOCSU1KAjx8/Pmpra6O1tXXAeGtra9TV1Q25ZsGCBfG73/0u3nvvvf6xX//61zFmzJiYMmXKkGsqKiqiqqpqwAMAAAAuZyV/BH3NmjXx9NNPx9atW+PgwYOxevXq6OjoiMbGxoj46Or18uXL++ffddddMWHChLjvvvviwIEDsXPnznjwwQfjr//6r+OKK64YuXcCAAAAl7CS7wO+bNmyOH78eGzcuDE6Oztj5syZ0dLSEtOmTYuIiM7Ozujo6Oif/0d/9EfR2toaf/M3fxNz5syJCRMmxNKlS+PRRx8duXcBAAAAl7iS7wN+MbgPOAAAAJku+n3AAQAAgOER4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkEOAAAACQQIADAABAAgEOAAAACQQ4AAAAJBDgAAAAkECAAwAAQAIBDgAAAAkEOAAAACQQ4AAAAJBAgAMAAEACAQ4AAAAJBDgAAAAkGFaAb9q0KaZPnx6VlZVRW1sbu3btOq91b7zxRpSXl8dXv/rV4fxYAAAAuGyVHODNzc2xatWqWL9+fbS3t0d9fX0sXLgwOjo6PnZdT09PLF++PL7xjW8Me7MAAABwuSoriqIoZcG8efNi9uzZsXnz5v6xGTNmxJIlS6Kpqemc6771rW/FjTfeGGPHjo1XX3019u/ff94/s7e3N6qrq6OnpyeqqqpK2S4AAACUbDQ6tKQr4CdPnoy2trZoaGgYMN7Q0BB79uw557pnnnkm3nrrrXjkkUfO6+f09fVFb2/vgAcAAABczkoK8O7u7jh9+nTU1NQMGK+pqYmurq4h1/zmN7+JtWvXxrZt26K8vPy8fk5TU1NUV1f3P6ZOnVrKNgEAAOCSM6w/wlZWVjbgeVEUg8YiIk6fPh133XVXbNiwIW666abzfv1169ZFT09P/+PIkSPD2SYAAABcMs7vkvT/NXHixBg7duygq93Hjh0bdFU8IuLEiROxb9++aG9vj+9973sREXHmzJkoiiLKy8tj+/btceuttw5aV1FRERUVFaVsDQAAAC5pJV0BHz9+fNTW1kZra+uA8dbW1qirqxs0v6qqKn71q1/F/v37+x+NjY3xpS99Kfbv3x/z5s27sN0DAADAZaKkK+AREWvWrIm777475syZE/Pnz4+f/vSn0dHREY2NjRHx0cfHf/vb38bPfvazGDNmTMycOXPA+muuuSYqKysHjQMAAMCnWckBvmzZsjh+/Hhs3LgxOjs7Y+bMmdHS0hLTpk2LiIjOzs5PvCc4AAAAfNaUfB/wi8F9wAEAAMh00e8DDgAAAAyPAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIMKwA37RpU0yfPj0qKyujtrY2du3adc65L7/8ctx+++3x+c9/PqqqqmL+/Pnx85//fNgbBgAAgMtRyQHe3Nwcq1ativXr10d7e3vU19fHwoULo6OjY8j5O3fujNtvvz1aWlqira0tbrnllli8eHG0t7df8OYBAADgclFWFEVRyoJ58+bF7NmzY/Pmzf1jM2bMiCVLlkRTU9N5vcaf/umfxrJly+Lhhx8+r/m9vb1RXV0dPT09UVVVVcp2AQAAoGSj0aElXQE/efJktLW1RUNDw4DxhoaG2LNnz3m9xpkzZ+LEiRNx9dVXn3NOX19f9Pb2DngAAADA5aykAO/u7o7Tp09HTU3NgPGampro6uo6r9f40Y9+FO+//34sXbr0nHOampqiurq6/zF16tRStgkAAACXnGH9EbaysrIBz4uiGDQ2lOeffz5+8IMfRHNzc1xzzTXnnLdu3bro6enpfxw5cmQ42wQAAIBLRnkpkydOnBhjx44ddLX72LFjg66K/2/Nzc2xYsWKeOGFF+K222772LkVFRVRUVFRytYAAADgklbSFfDx48dHbW1ttLa2DhhvbW2Nurq6c657/vnn4957743nnnsu7rzzzuHtFAAAAC5jJV0Bj4hYs2ZN3H333TFnzpyYP39+/PSnP42Ojo5obGyMiI8+Pv7b3/42fvazn0XER/G9fPny+Od//uf42te+1n/1/Iorrojq6uoRfCsAAABw6So5wJctWxbHjx+PjRs3RmdnZ8ycOTNaWlpi2rRpERHR2dk54J7gP/nJT+LUqVPx3e9+N7773e/2j99zzz3x7LPPXvg7AAAAgMtAyfcBvxjcBxwAAIBMF/0+4AAAAMDwCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAEAhwAAAASCHAAAABIIMABAAAggQAHAACABMMK8E2bNsX06dOjsrIyamtrY9euXR87f8eOHVFbWxuVlZVxww03xFNPPTWszQIAAMDlquQAb25ujlWrVsX69eujvb096uvrY+HChdHR0THk/MOHD8eiRYuivr4+2tvb46GHHoqVK1fGSy+9dMGbBwAAgMtFWVEURSkL5s2bF7Nnz47Nmzf3j82YMSOWLFkSTU1Ng+Z///vfj9deey0OHjzYP9bY2Bi//OUvY+/evef1M3t7e6O6ujp6enqiqqqqlO0CAABAyUajQ8tLmXzy5Mloa2uLtWvXDhhvaGiIPXv2DLlm79690dDQMGDsjjvuiC1btsSHH34Y48aNG7Smr68v+vr6+p/39PRExEf/BQAAAMBoO9ufJV6z/lglBXh3d3ecPn06ampqBozX1NREV1fXkGu6urqGnH/q1Kno7u6OSZMmDVrT1NQUGzZsGDQ+derUUrYLAAAAF+T48eNRXV09Iq9VUoCfVVZWNuB5URSDxj5p/lDjZ61bty7WrFnT//z3v/99TJs2LTo6OkbsjcOlpre3N6ZOnRpHjhzxVQs+tZxzPguccz4LnHM+C3p6euL666+Pq6++esRes6QAnzhxYowdO3bQ1e5jx44Nusp91rXXXjvk/PLy8pgwYcKQayoqKqKiomLQeHV1tf+B86lXVVXlnPOp55zzWeCc81ngnPNZMGbMyN29u6RXGj9+fNTW1kZra+uA8dbW1qirqxtyzfz58wfN3759e8yZM2fI738DAADAp1HJKb9mzZp4+umnY+vWrXHw4MFYvXp1dHR0RGNjY0R89PHx5cuX989vbGyMt99+O9asWRMHDx6MrVu3xpYtW+KBBx4YuXcBAAAAl7iSvwO+bNmyOH78eGzcuDE6Oztj5syZ0dLSEtOmTYuIiM7OzgH3BJ8+fXq0tLTE6tWr48knn4zJkyfH448/Ht/85jfP+2dWVFTEI488MuTH0uHTwjnns8A557PAOeezwDnns2A0znnJ9wEHAAAASjdy3yYHAAAAzkmAAwAAQAIBDgAAAAkEOAAAACS4ZAJ806ZNMX369KisrIza2trYtWvXx87fsWNH1NbWRmVlZdxwww3x1FNPJe0Uhq+Uc/7yyy/H7bffHp///Oejqqoq5s+fHz//+c8TdwvDU+rv87PeeOONKC8vj69+9auju0EYAaWe876+vli/fn1MmzYtKioq4otf/GJs3bo1abcwPKWe823btsWsWbPiyiuvjEmTJsV9990Xx48fT9otlGbnzp2xePHimDx5cpSVlcWrr776iWtGokEviQBvbm6OVatWxfr166O9vT3q6+tj4cKFA25n9v87fPhwLFq0KOrr66O9vT0eeuihWLlyZbz00kvJO4fzV+o537lzZ9x+++3R0tISbW1tccstt8TixYujvb09eedw/ko952f19PTE8uXL4xvf+EbSTmH4hnPOly5dGv/2b/8WW7Zsif/6r/+K559/Pm6++ebEXUNpSj3nu3fvjuXLl8eKFSvizTffjBdeeCF+8YtfxP3335+8czg/77//fsyaNSueeOKJ85o/Yg1aXALmzp1bNDY2Dhi7+eabi7Vr1w45/+/+7u+Km2++ecDYt7/97eJrX/vaqO0RLlSp53woX/7yl4sNGzaM9NZgxAz3nC9btqz4+7//++KRRx4pZs2aNYo7hAtX6jn/13/916K6uro4fvx4xvZgRJR6zv/xH/+xuOGGGwaMPf7448WUKVNGbY8wUiKieOWVVz52zkg16EW/An7y5Mloa2uLhoaGAeMNDQ2xZ8+eIdfs3bt30Pw77rgj9u3bFx9++OGo7RWGazjn/H87c+ZMnDhxIq6++urR2CJcsOGe82eeeSbeeuuteOSRR0Z7i3DBhnPOX3vttZgzZ0788Ic/jOuuuy5uuummeOCBB+IPf/hDxpahZMM553V1dXH06NFoaWmJoijinXfeiRdffDHuvPPOjC3DqBupBi0f6Y2Vqru7O06fPh01NTUDxmtqaqKrq2vINV1dXUPOP3XqVHR3d8ekSZNGbb8wHMM55//bj370o3j//fdj6dKlo7FFuGDDOee/+c1vYu3atbFr164oL7/o/0iCTzScc37o0KHYvXt3VFZWxiuvvBLd3d3xne98J959913fA+eSNJxzXldXF9u2bYtly5bF//zP/8SpU6fiL/7iL+LHP/5xxpZh1I1Ug170K+BnlZWVDXheFMWgsU+aP9Q4XEpKPednPf/88/GDH/wgmpub45prrhmt7cGION9zfvr06bjrrrtiw4YNcdNNN2VtD0ZEKb/Pz5w5E2VlZbFt27aYO3duLFq0KB577LF49tlnXQXnklbKOT9w4ECsXLkyHn744Whra4vXX389Dh8+HI2NjRlbhRQj0aAX/XLDxIkTY+zYsYP+bdqxY8cG/RuGs6699toh55eXl8eECRNGba8wXMM552c1NzfHihUr4oUXXojbbrttNLcJF6TUc37ixInYt29ftLe3x/e+972I+ChUiqKI8vLy2L59e9x6660pe4fzNZzf55MmTYrrrrsuqqur+8dmzJgRRVHE0aNH48YbbxzVPUOphnPOm5qaYsGCBfHggw9GRMRXvvKVuOqqq6K+vj4effRRn1DlsjdSDXrRr4CPHz8+amtro7W1dcB4a2tr1NXVDblm/vz5g+Zv37495syZE+PGjRu1vcJwDeecR3x05fvee++N5557zneouOSVes6rqqriV7/6Vezfv7//0djYGF/60pdi//79MW/evKytw3kbzu/zBQsWxO9+97t47733+sd+/etfx5gxY2LKlCmjul8YjuGc8w8++CDGjBmYFmPHjo2I/3eVEC5nI9agJf3JtlHyL//yL8W4ceOKLVu2FAcOHChWrVpVXHXVVcV///d/F0VRFGvXri3uvvvu/vmHDh0qrrzyymL16tXFgQMHii1bthTjxo0rXnzxxYv1FuATlXrOn3vuuaK8vLx48skni87Ozv7H73//+4v1FuATlXrO/zd/BZ3LQann/MSJE8WUKVOKv/zLvyzefPPNYseOHcWNN95Y3H///RfrLcAnKvWcP/PMM0V5eXmxadOm4q233ip2795dzJkzp5g7d+7FegvwsU6cOFG0t7cX7e3tRUQUjz32WNHe3l68/fbbRVGMXoNeEgFeFEXx5JNPFtOmTSvGjx9fzJ49u9ixY0f/f3bPPfcUX//61wfM/4//+I/iz/7sz4rx48cXX/jCF4rNmzcn7xhKV8o5//rXv15ExKDHPffck79xKEGpv8//fwKcy0Wp5/zgwYPFbbfdVlxxxRXFlClTijVr1hQffPBB8q6hNKWe88cff7z48pe/XFxxxRXFpEmTir/6q78qjh49mrxrOD///u///rH/X3u0GrSsKHwmBAAAAEbbRf8OOAAAAHwWCHAAAABIIMABAAAggQAHAACABAIcAAAAEghwAAAASCDAAQAAIIEABwAAgAQCHAAAABIIcAAAAEggwAEAACCBAAcAAIAE/wdPFo57AkoCnQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_TA, std_TA = return_Mean_STD_forPSTH(TA_traj_list)\n",
    "mean_HA, std_HA = return_Mean_STD_forPSTH(HA_traj_list)\n",
    "mean_Ridge, std_Ridge = return_Mean_STD_forPSTH(Ridge_traj_list)\n",
    "\n",
    "mean_TA = mean_TA*8.15\n",
    "mean_HA=mean_HA*40\n",
    "mean_Ridge = mean_Ridge*25*5.5\n",
    "\n",
    "means = [mean_TA, mean_Ridge, mean_HA, mean_HA+mean_TA]\n",
    "stds = [std_TA*8.15, std_Ridge*25*5.5, std_HA*40, std_TA*8.15]\n",
    "\n",
    "_, ax = plt.subplots(figsize=(12, 8))\n",
    "clrs = sns.color_palette(\"husl\", 5)\n",
    "\n",
    "plot_PSTH_Mean_STD_label_color_pre_assigned(means, stds, ax)\n",
    "ax.set_xlabel('Time (s)', fontsize=18)\n",
    "ax.set_ylabel('Angular Momentum (g-cm2/s)', fontsize=18)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14) \n",
    "plt.savefig('out.svg', format='svg', dpi=1200)\n",
    "print(np.trapz(mean_TA[60:110]), np.trapz(mean_HA[60:110]), np.trapz(mean_Ridge[60:110]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TA_traj_list_AngMom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(TA_traj_list_AngMom, HA_traj_list_AngMom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot BodyAxis for small swing (red) and big swing (green)\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(15,10))\n",
    "small_swing_list = []\n",
    "medium_swing_list = []\n",
    "big_swing_list = []\n",
    "for i in np.arange(len(Theta_theta_prime_list)):\n",
    "    var_to_measure = Theta_theta_prime_list[i][8]\n",
    "    if Theta_theta_prime_list[i][0] < 180:# and all(i < 50 for i in var_to_measure):\n",
    "        #print(len(var_to_measure))\n",
    "        ax1.plot(var_to_measure, 'r', lw=2, alpha = 0.3)\n",
    "        if len(var_to_measure) == 259:\n",
    "            small_swing_list.append(var_to_measure) \n",
    "    elif Theta_theta_prime_list[i][0] >180 and Theta_theta_prime_list[i][4] < 180:# and all(i < 50 for i in var_to_measure):\n",
    "        ax1.plot(var_to_measure, 'g', lw=2, alpha = 0.3)\n",
    "        if len(var_to_measure) == 259:\n",
    "            medium_swing_list.append(var_to_measure) \n",
    "    elif Theta_theta_prime_list[i][0] > 180 and Theta_theta_prime_list[i][4] > 180:#  and all(i < 50 for i in var_to_measure):\n",
    "        ax1.plot(var_to_measure, 'b', lw=2, alpha = 0.08)\n",
    "        ax1.axvline(100,0,360, color = 'red')\n",
    " #       ax1.set_ylim(70, 120)\n",
    "        ax1.set_ylim(230,310)\n",
    "        if len(var_to_measure) == 259:\n",
    "            big_swing_list.append(var_to_measure) \n",
    "plt.savefig('sample.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot COMY velocity for small swing (red) and big swing (green)\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(15,10))\n",
    "small_swing_list = []\n",
    "big_swing_list = []\n",
    "for i in np.arange(len(Theta_theta_prime_list)):\n",
    "    if Theta_theta_prime_list[i][0] < 180:\n",
    "        ax1.plot(smooth(Theta_theta_prime_list[i][3], 10), 'r', lw=2, alpha = 0.3)\n",
    "#         if len(Theta_theta_prime_list[i][11]) == 258:\n",
    "#             small_swing_list.append(Theta_theta_prime_list[i][9]) \n",
    "    elif Theta_theta_prime_list[i][0] >180 and Theta_theta_prime_list[i][4] < 220:\n",
    "        ax1.plot(smooth(Theta_theta_prime_list[i][3], 10), 'g', lw=2, alpha = 0.3)\n",
    "#         if len(Theta_theta_prime_list[i][11]) == 258:\n",
    "#             small_swing_list.append(Theta_theta_prime_list[i][9]) \n",
    "    elif Theta_theta_prime_list[i][0] > 180 and Theta_theta_prime_list[i][4] > 220:\n",
    "        ax1.plot(smooth(Theta_theta_prime_list[i][3], 10), 'b', lw=2, alpha = 0.1)\n",
    "        ax1.axvline(100,0,360, color = 'red')\n",
    "#        plt.savefig('sample.png')\n",
    "#         ax1.set_xlim(200, 250)\n",
    "#         ax1.set_ylim(-2,3)\n",
    "#         if len(Theta_theta_prime_list[i][9]) == 258:\n",
    "#             big_swing_list.append(Theta_theta_prime_list[i][9]) \n",
    "\n",
    "plt.savefig('sample.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot COMX for small swing (red) and big swing (green)\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(15,10))\n",
    "small_swing_list = []\n",
    "big_swing_list = []\n",
    "for i in np.arange(len(Theta_theta_prime_list)):\n",
    "    if Theta_theta_prime_list[i][0] < 180:\n",
    "        ax1.plot(Theta_theta_prime_list[i][11], 'r', lw=2, alpha = 0.3)\n",
    "#         if len(Theta_theta_prime_list[i][11]) == 258:\n",
    "#             small_swing_list.append(Theta_theta_prime_list[i][9]) \n",
    "    elif Theta_theta_prime_list[i][0] >180 and Theta_theta_prime_list[i][4] < 180:\n",
    "        ax1.plot(Theta_theta_prime_list[i][11], 'g', lw=2, alpha = 0.3)\n",
    "#         if len(Theta_theta_prime_list[i][11]) == 258:\n",
    "#             small_swing_list.append(Theta_theta_prime_list[i][9]) \n",
    "    elif Theta_theta_prime_list[i][0] > 180 and Theta_theta_prime_list[i][4] > 180:\n",
    "        ax1.plot(Theta_theta_prime_list[i][11], 'b', lw=2, alpha = 0.1)\n",
    "#         ax1.set_xlim(200, 250)\n",
    "#         ax1.set_ylim(-2,3)\n",
    "#         if len(Theta_theta_prime_list[i][9]) == 258:\n",
    "#             big_swing_list.append(Theta_theta_prime_list[i][9]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_PSTH_Mean_STD_label_color_pre_assigned(mean_array, std_array, ax):\n",
    "clrs = sns.color_palette(\"husl\", len(mean_array))\n",
    "label_strings = ['Down 4mm', 'Up 4mm']#, 'Ipsi 4mm', 'Contra 4mm', 'Ipsi 4mm', 'Contra 4mm']\n",
    "#    _, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    for i in range(len(mean_array)):\n",
    "        epochs = list(range(len(mean_array[i])))\n",
    "        ax.plot(epochs, mean_array[i], c=clrs[i], label = label_strings[i])\n",
    "        ax.fill_between(epochs, mean_array[i]-std_array[i], mean_array[i]+std_array[i] ,alpha=0.3, facecolor=clrs[i])\n",
    "        ax.legend(loc=\"lower left\")\n",
    "        ax.set_xlabel('Frame number (FR = 300 Hz)')\n",
    "        ax.set_ylabel('Angle (degree)')\n",
    "        ax.axvline(60,0,360, color = 'red')\n",
    "        ax.axvline(100,0,360, color = 'red')\n",
    "\n",
    "return ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PSTH_Mean_STD_label_color_pre_assigned(mean_array, std_array, ax):\n",
    "    clrs = sns.color_palette(\"husl\", len(mean_array))\n",
    "    label_strings = ['SS', 'MS', 'BS']#, 'Ipsi 4mm', 'Contra 4mm', 'Ipsi 4mm', 'Contra 4mm']\n",
    "#    _, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    with sns.axes_style(\"darkgrid\"):\n",
    "        for i in range(len(mean_array)):\n",
    "            epochs = list(range(len(mean_array[i])))\n",
    "            ax.plot(epochs, mean_array[i], c=clrs[i], label = label_strings[i])\n",
    "            ax.fill_between(epochs, mean_array[i]-std_array[i], mean_array[i]+std_array[i] ,alpha=0.3, facecolor=clrs[i])\n",
    "            ax.legend(loc=\"lower left\")\n",
    "            ax.set_xlabel('Frame number (FR = 300 Hz)')\n",
    "            ax.set_ylabel('Angle (degree)')\n",
    "            ax.axvline(100,0,360, color = 'red')\n",
    "            #ax.axvline(140,0,360, color = 'red')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_SS, std_SS = return_Mean_STD_forPSTH(small_swing_list)\n",
    "mean_MS, std_MS = return_Mean_STD_forPSTH(medium_swing_list)\n",
    "mean_BS, std_BS = return_Mean_STD_forPSTH(big_swing_list)\n",
    "means = [mean_SS, mean_MS, mean_BS]\n",
    "stds = [std_SS, std_MS, std_BS]\n",
    "\n",
    "_, ax = plt.subplots(figsize=(12, 8))\n",
    "clrs = sns.color_palette(\"husl\", 5)\n",
    "\n",
    "plot_PSTH_Mean_STD_label_color_pre_assigned(means, stds, ax)\n",
    "# ax.set_xlim(50,200)\n",
    "# ax.set_ylim(-1,3)\n",
    "plt.savefig('sample.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1, figsize=(15,6))\n",
    "for i in np.arange(len(Theta_theta_prime_list)):\n",
    "#    print(Theta_theta_prime_list[i][4])\n",
    "    if Theta_theta_prime_list[i][4] > 180:\n",
    "        #fig, ax1 = plt.subplots(1, 1, figsize=(15,6))\n",
    "        #plt.scatter(Theta_theta_prime_list[i][1], Theta_theta_prime_list[i][2], c = 'b')\n",
    "        #ax1.plot(Theta_theta_prime_list[i][3], 'b')\n",
    "        #ax1.plot(np.diff(Theta_theta_prime_list[i][5]), 'g')\n",
    "        ax1.plot(np.diff(Theta_theta_prime_list[i][6]), 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot data in 3D binned wrt type of tail response\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in np.arange(len(Theta_theta_prime_list)):\n",
    "#    print(Theta_theta_prime_list[i][4])\n",
    "    if Theta_theta_prime_list[i][0] < 50000:\n",
    "        #print(Theta_theta_prime_list[i][1], Theta_theta_prime_list[i][2] )\n",
    "        ax.scatter(Theta_theta_prime_list[i][2], Theta_theta_prime_list[i][1], \\\n",
    "                      Theta_theta_prime_list[i][0], c = 'b')\n",
    "#     elif Theta_theta_prime_list[i][0] > 180 and Theta_theta_prime_list[i][4] < 250:\n",
    "#         #print(Theta_theta_prime_list[i][1], Theta_theta_prime_list[i][2] )\n",
    "#         ax.scatter(Theta_theta_prime_list[i][2], Theta_theta_prime_list[i][1], \\\n",
    "#                       Theta_theta_prime_list[i][0], c = 'g')\n",
    "#     elif Theta_theta_prime_list[i][0] > 180 and Theta_theta_prime_list[i][4] > 250:\n",
    "#         #print(Theta_theta_prime_list[i][1], Theta_theta_prime_list[i][2] )\n",
    "#         ax.scatter(Theta_theta_prime_list[i][2], Theta_theta_prime_list[i][1], \\\n",
    "#                       Theta_theta_prime_list[i][0], c = 'r')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use SVD to calculate best fitting line in 3D data\n",
    "\n",
    "\n",
    "x = [item[2] for item in Theta_theta_prime_list] \n",
    "y = [item[1] for item in Theta_theta_prime_list]\n",
    "z = [item[0] for item in Theta_theta_prime_list]\n",
    "\n",
    "#Mean corrected\n",
    "x = x-np.nanmean(x)\n",
    "y = y-np.nanmean(y)\n",
    "z = z-np.nanmean(z)\n",
    "\n",
    "\n",
    "data = np.stack((x,y,z),axis = 1)\n",
    "\n",
    "#data = data[0:20]\n",
    "\n",
    "# Calculate the mean of the points, i.e. the 'center' of the cloud\n",
    "datamean = np.nanmean(data, axis=0)\n",
    "\n",
    "#dropped nan\n",
    "\n",
    "data = np.array([x for x in data if ~np.isnan(np.mean(x))])\n",
    "\n",
    "# Do an SVD on the mean-centered data.\n",
    "uu, dd, vv = np.linalg.svd(data)# - datamean)\n",
    "\n",
    "# # Now vv[0] contains the first principal component, i.e. the direction\n",
    "# # vector of the 'best fit' line in the least squares sense.\n",
    "\n",
    "# # Now generate some points along this best fit line, for plotting.\n",
    "\n",
    "# # I use -7, 7 since the spread of the data is roughly 14\n",
    "# # and we want it to have mean 0 (like the points we did\n",
    "# # the svd on). Also, it's a straight line, so we only need 2 points.\n",
    "linepts = vv[0] * np.mgrid[-100:100:2j][:, np.newaxis]\n",
    "\n",
    "# shift by the mean to get the line in the right place\n",
    "linepts += datamean\n",
    "\n",
    "# Verify that everything looks right.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d as m3d\n",
    "\n",
    "ax = m3d.Axes3D(plt.figure())\n",
    "ax.scatter3D(*data.T)\n",
    "ax.plot3D(*linepts.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "#fit regression model\n",
    "model = LinearRegression()\n",
    "x = [item[2] for item in Theta_theta_prime_list] \n",
    "y = [item[1] for item in Theta_theta_prime_list]\n",
    "z = [item[0] for item in Theta_theta_prime_list]\n",
    "#Mean corrected\n",
    "# x = x-np.nanmean(x)\n",
    "# y = y-np.nanmean(y)\n",
    "# z = z-np.nanmean(z)\n",
    "data = np.stack((x,y,z),axis = 1)\n",
    "# Calculate the mean of the points, i.e. the 'center' of the cloud\n",
    "datamean = np.nanmean(data, axis=0)\n",
    "#dropped nan\n",
    "data = np.array([x for x in data if ~np.isnan(np.mean(x))])\n",
    "# x = np.array([item[0] for item in data])\n",
    "# y = np.array([item[1] for item in data])\n",
    "# z = np.array([item[2] for item in data])\n",
    "\n",
    "# #Reshape\n",
    "# x = x.reshape(-1, 1)\n",
    "# y = y.reshape(-1, 1)\n",
    "# z = z.reshape(-1, 1)\n",
    "\n",
    "# bla = np.concatenate(x, axis = 0)\n",
    "# bla2 = np.concatenate(y, axis = 0)\n",
    "# bla3 = np.concatenate(z, axis = 0)\n",
    "#model.fit(x, y)\n",
    "\n",
    "#display adjusted R-squared\n",
    "#1 - (1-model.score(x, y))*(len(y)-1)/(len(y)-x.shape[1]-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(5, 6))\n",
    "sns.set_theme(color_codes=False)\n",
    "#tips = sns.load_dataset(\"tips\")\n",
    "#data_bla = np.concatenate((bla, bla),axis = 0)\n",
    "\n",
    "sns.regplot(x=z, y=x)#, data=data_bla);\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_quadrant_classifier_equal_number(TA, no_quad, list_of_TAMeans_equally_split):\n",
    "    classifier = []\n",
    "    #print(len(list_of_TAMeans_equally_split))\n",
    "    for i in np.arange(len(list_of_TAMeans_equally_split)):\n",
    "        first_value = list_of_TAMeans_equally_split[i][0]\n",
    "        last_value = list_of_TAMeans_equally_split[i][-1]\n",
    "        if first_value <= TA <= last_value:\n",
    "            classifier = i\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Split traces into N equally populated arrays\n",
    "dict_ridge_4mm = dict_ridge_all['all']\n",
    "dict_traces_divided_on_TA_at_pert_time = defaultdict(dict)\n",
    "class_value_list = []\n",
    "TA_end_PSTH_list_array = np.array(TA_end_PSTH_list)\n",
    "TA_end_PSTH_list_woutNaN = TA_end_PSTH_list_array[~numpy.isnan(TA_end_PSTH_list_array)]\n",
    "TA_end_PSTH_list_woutNaN.sort()\n",
    "list_of_TAMeans_eq_split = splitsequenceequally(TA_end_PSTH_list_woutNaN,no_quad_to_plot)\n",
    "for i in np.arange(len(search_key)):\n",
    "    res = dict(filter(lambda item: search_key[i] in item[0], dict_ridge_4mm.items())) \n",
    "    res = excludeTATracesHighDerivative(res)\n",
    "    res = excludeTATracesabove180(res)\n",
    "    values_filename_list = list(res.values())\n",
    "    key_filename_list = list(res.keys()) \n",
    "    for i in np.arange(len(key_filename_list)):\n",
    "        Trial_classvalue = values_filename_list[i][7]\n",
    "        TA_traj = values_filename_list[i][-3]\n",
    "        if Trial_classvalue == 1:\n",
    "            TA_at_pert_time = np.mean(TA_traj[100:105])\n",
    "            quadrant_value = assign_quadrant_classifier_equal_number(TA_at_pert_time, no_quad_to_plot, list_of_TAMeans_eq_split)\n",
    "            class_value_list.append([TA_traj, quadrant_value])\n",
    "\n",
    "    dict_traces_divided_on_TA_at_pert_time[key_filename_list[i]] = class_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Assign TA list for each quadrant in dict\n",
    "values_list = list(dict_traces_divided_on_TA_at_pert_time.values())\n",
    "key_list = list(dict_traces_divided_on_TA_at_pert_time.keys()) \n",
    "dict_mean_quadrants = defaultdict(dict)\n",
    "for k in range(no_quad_to_plot):\n",
    "    TA_traj_list = []\n",
    "    for i in np.arange(len(key_list)):\n",
    "        for j in np.arange(len(values_list[i])):\n",
    "            TA_traj = values_list[i][j][0]\n",
    "            quad_class = values_list[i][j][1]\n",
    "            if quad_class == k and len(TA_traj):# == 169:\n",
    "                TA_traj_list.append(TA_traj)\n",
    "    dict_mean_quadrants[k] = TA_traj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot quadrants\n",
    "values_list = list(dict_mean_quadrants.values())\n",
    "key_list = list(dict_mean_quadrants.keys()) \n",
    "\n",
    "for i in np.arange(len(key_list)):\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(15,6))\n",
    "    values_quad = values_list[i]\n",
    "    for j in np.arange(len(values_quad)):\n",
    "        ax1.plot(values_quad[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "values_list = list(dict_mean_quadrants.values())\n",
    "key_list = list(dict_mean_quadrants.keys()) \n",
    "color_idx = np.linspace(0, 1, no_quad_to_plot)\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(15,10))\n",
    "\n",
    "for color_i, i in zip(color_idx, np.arange(no_quad_to_plot)):#len(key_list)):\n",
    "    mean_quadrant_traces = np.nanmean(dict_mean_quadrants[i], axis=0)\n",
    "    data = mean_quadrant_traces#-mean_quadrant_traces[50]\n",
    "    #Plot here\n",
    "    plt.plot(data, color=plt.cm.coolwarm(color_i), lw=4, alpha = 0.7, label=\"%s quadrant\" % (i+1))\n",
    "    plt.legend()\n",
    "    plt.xlabel('Frame number (FR = 300 Hz)', size = 15)\n",
    "    plt.ylabel('Tail Angle (degree)', size = 15)\n",
    "    plt.axvline(60,0,360, color = 'red')\n",
    "    plt.axvline(100,0,360, color = 'red')\n",
    "    #plt.xlim(30,120)\n",
    "    plt.savefig('sample.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "values_list = list(dict_mean_quadrants.values())\n",
    "key_list = list(dict_mean_quadrants.keys()) \n",
    "color_idx = np.linspace(0, 1, no_quad_to_plot)\n",
    "#fig, ax1 = plt.subplots(1, 1, figsize=(15,6))\n",
    "for color_i, i in zip(color_idx, np.arange(len(values_list))):#len(key_list)):\n",
    "    #fig, ax1 = plt.subplots(1, 1, figsize=(15,6))\n",
    "\n",
    "    mean_quadrant_traces = np.nanmean(dict_mean_quadrants[i], axis=0)\n",
    "    if isinstance(mean_quadrant_traces, float):\n",
    "        print(mean_quadrant_traces)\n",
    "    else:\n",
    "        mean_quadrant_traces_der = np.diff(mean_quadrant_traces)\n",
    "        velocity_at_pert_time = (mean_quadrant_traces[60]-mean_quadrant_traces[55])/6\n",
    "        ax.scatter(mean_quadrant_traces[60], velocity_at_pert_time, mean_quadrant_traces[100], color=plt.cm.coolwarm(color_i), lw=3, alpha = 0.5)\n",
    "\n",
    "        #ax.xlabel('Tail Angle (degree)', size = 13)\n",
    "        #ax.ylabel('Tail Velocity (degree/timepoint)', size = 13)\n",
    "        #plt.savefig('sample.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####CHECKED INDEXES IN DICT_ALL UNTIL HERE##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter of TA and TA derivative to check that the tail response depends on the position of the tail and\n",
    "#its velocity\n",
    "#Plot trials of same width together changing the dict_ridge_all key\n",
    "search_key = ['M48', 'M49', 'M50', 'M51', 'M52', 'M53', 'M54', 'M55', 'M56', 'M57', 'M58', 'M59', 'M60', 'M61', 'M62']\n",
    "dict_ridge_4mm = dict_ridge_all['4mm']\n",
    "\n",
    "no_quad_to_plot = 20\n",
    "dict_traces_divided_on_TA_at_pert_time = defaultdict(dict)\n",
    "\n",
    "class_value_list = []\n",
    "#fig, ax1 = plt.subplots(1, 1, figsize=(15,6))\n",
    "fig = plt.figure(figsize=(12,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for i in np.arange(len(search_key)):\n",
    "    res = dict(filter(lambda item: search_key[i] in item[0], dict_ridge_4mm.items())) \n",
    "    res = excludeTATracesHighDerivative(res)\n",
    "#    res = flip_traces_above_180(res)\n",
    "    values_filename_list = list(res.values())\n",
    "    key_filename_list = list(res.keys()) \n",
    "#    fig, ax1 = plt.subplots(1, 1, figsize=(15,6))\n",
    "#    fig = plt.figure()\n",
    "    for color_i, i in zip(color_idx,np.arange(len(key_filename_list))):\n",
    "#        print(key_filename_list)\n",
    "        Trial_classvalue = values_filename_list[i][7]\n",
    "        TA_traj = smooth(values_filename_list[i][-1], 20)\n",
    "        TA_traj_der = np.diff(TA_traj)\n",
    "        Ridge_traj = values_filename_list[i][9]\n",
    "        n = 7 #column in dict with trial classifier\n",
    "        dict_trial_classifier = {k:v[n] for k,v in res.items()}\n",
    "        dict_trial_classifier_value = list(dict_trial_classifier.values())\n",
    "        count_ipsi_trials = dict_trial_classifier_value.count(1)\n",
    "        #print(len(TA_traj[0:-1]), len(TA_traj_der))\n",
    "        if Trial_classvalue == 1:# and len(TA_traj) == 169:\n",
    "            #plt.scatter(TA_traj[0:-1], TA_traj_der)\n",
    "            z =  list(range(0, 168))\n",
    "            ax.scatter(z, TA_traj[0:-1], TA_traj_der, color=plt.cm.coolwarm(color_i), marker='o')\n",
    "plt.show()\n",
    "            #plt.plot(Ridge_traj, 'k')\n",
    "            #plt.ylim(110, 250)\n",
    "#             #plt.xlim(55, 98)\n",
    "#             TA_at_pert_time = np.mean(TA_traj[55:60])\n",
    "#             quadrant_value = assign_quadrant_classifier(TA_at_pert_time, no_quad_to_plot)\n",
    "# #             class_value_list.append([smooth(TA_traj,20), quadrant_value])\n",
    "# #                 #print(quadrant_value)\n",
    "# #     dict_traces_divided_on_TA_at_pert_time[key_filename_list[i]] = class_value_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write function to extract only ipsi trials from dict_all specific width and asign it to array\n",
    "def return_Ipsi_trials_array(width):\n",
    "    dict_ridge_4mm = dict_ridge_all[width]\n",
    "    search_key = ['M48', 'M49', 'M50', 'M51', 'M52', 'M53', 'M54', 'M55', 'M56', 'M57', 'M58', 'M59', 'M60', 'M61', 'M62']\n",
    "\n",
    "    TA_traj_4mm_ipsi = []\n",
    "    for i in np.arange(len(search_key)):\n",
    "        res = dict(filter(lambda item: search_key[i] in item[0], dict_ridge_4mm.items())) \n",
    "        values_filename_list = list(res.values())\n",
    "        key_filename_list = list(res.keys())   \n",
    "        for i in np.arange(len(key_filename_list)):\n",
    "            Trial_classvalue = values_filename_list[i][7]\n",
    "            TA_traj = values_filename_list[i][10]\n",
    "            if Trial_classvalue == 1 and len(TA_traj) == 150: ###BE CAREFUL HERE IF I CHANGE WIDTH OF PSTH:\n",
    "                TA_traj_4mm_ipsi.append(TA_traj)\n",
    "    return TA_traj_4mm_ipsi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write function to extract only contra trials from dict_all specific width and asign it to array\n",
    "def return_Contra_trials_array(width):\n",
    "    dict_ridge_4mm = dict_ridge_all[width]\n",
    "    search_key = ['M48', 'M49', 'M50', 'M51', 'M52', 'M53', 'M54', 'M55', 'M56', 'M57', 'M58', 'M59', 'M60', 'M61', 'M62']\n",
    "\n",
    "    TA_traj_4mm_contra = []\n",
    "    for i in np.arange(len(search_key)):\n",
    "        res = dict(filter(lambda item: search_key[i] in item[0], dict_ridge_4mm.items())) \n",
    "        values_filename_list = list(res.values())\n",
    "        key_filename_list = list(res.keys())   \n",
    "        for i in np.arange(len(key_filename_list)):\n",
    "            Trial_classvalue = values_filename_list[i][7]\n",
    "            TA_traj = values_filename_list[i][10]\n",
    "            if Trial_classvalue == -1 and len(TA_traj) == 150: ###BE CAREFUL HERE IF I CHANGE WIDTH OF PSTH\n",
    "                TA_traj_4mm_contra.append(TA_traj)\n",
    "    return TA_traj_4mm_contra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide ipsi 4 mm traces based on whether they are going down or up\n",
    "def Divide_TATraces_UpVSDown(TA_traj_4mm_ipsi):\n",
    "    TA_traces_goingDOWN = []\n",
    "    TA_traces_goingUP = []\n",
    "\n",
    "    for i in np.arange(len(TA_traj_4mm_ipsi)):\n",
    "        TA_chunk = TA_traj_4mm_ipsi[i]\n",
    "        TA__chunk_afterTilt = TA_chunk[120:140]\n",
    "        TA__chunk_beforeTilt = TA_chunk[0:60]\n",
    "        TA__chunk_afterTilt_mean = np.nanmean(TA__chunk_afterTilt)\n",
    "        TA__chunk_beforeTilt_mean = np.nanmean(TA__chunk_beforeTilt)\n",
    "        thresh_trial_down = 180\n",
    "        thresh_trial_up = 270\n",
    "\n",
    "        if 0 < TA__chunk_afterTilt_mean < 150:\n",
    "            TA_traces_goingDOWN.append(TA_chunk)\n",
    "        elif 240 < TA__chunk_afterTilt_mean <360 and TA__chunk_beforeTilt_mean < 180:\n",
    "            TA_traces_goingUP.append(TA_chunk)\n",
    "    return TA_traces_goingDOWN, TA_traces_goingUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PSTH_Mean_STD(mean_array, std_array, ax, label_strings, color):\n",
    "#    clrs = sns.color_palette(\"husl\", len(mean_array))\n",
    "#    label_strings = ['Down 4mm', 'Up 4mm', 'Ipsi 4mm', 'Contra 4mm', 'Ipsi 4mm', 'Contra 4mm']\n",
    "#    _, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    with sns.axes_style(\"darkgrid\"):\n",
    "        for i in range(len(mean_array)):\n",
    "            epochs = list(range(len(mean_array[i])))\n",
    "            ax.plot(epochs, mean_array[i], c=color, label = label_strings[i])\n",
    "            ax.fill_between(epochs, mean_array[i]-std_array[i], mean_array[i]+std_array[i] ,alpha=0.3, facecolor=color)\n",
    "            ax.legend(loc=\"lower left\")\n",
    "            ax.set_xlabel('Frame number (FR = 300 Hz)')\n",
    "            ax.set_ylabel('Angle (degree)')\n",
    "            ax.axvline(60,0,360, color = 'red')\n",
    "            ax.axvline(100,0,360, color = 'red')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PSTH_Mean_STD_label_color_pre_assigned(mean_array, std_array, ax):\n",
    "    clrs = sns.color_palette(\"husl\", len(mean_array))\n",
    "    label_strings = ['Down 4mm', 'Up 4mm']#, 'Ipsi 4mm', 'Contra 4mm', 'Ipsi 4mm', 'Contra 4mm']\n",
    "#    _, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    with sns.axes_style(\"darkgrid\"):\n",
    "        for i in range(len(mean_array)):\n",
    "            epochs = list(range(len(mean_array[i])))\n",
    "            ax.plot(epochs, mean_array[i], c=clrs[i], label = label_strings[i])\n",
    "            ax.fill_between(epochs, mean_array[i]-std_array[i], mean_array[i]+std_array[i] ,alpha=0.3, facecolor=clrs[i])\n",
    "            ax.legend(loc=\"lower left\")\n",
    "            ax.set_xlabel('Frame number (FR = 300 Hz)')\n",
    "            ax.set_ylabel('Angle (degree)')\n",
    "            ax.axvline(60,0,360, color = 'red')\n",
    "            ax.axvline(100,0,360, color = 'red')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_mean_and_derivatives(array_value_dict):\n",
    "    mean_array = smooth(np.nanmean(array_value_dict, axis = 0), 10)   \n",
    "    mean_array_1st_der = np.diff(mean_array)\n",
    "    mean_array_2nd_der = np.diff(mean_array_1st_der)\n",
    "    return mean_array, mean_array_1st_der, mean_array_2nd_der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_STD_and_derivatives(array_value_dict):\n",
    "    STD_array = smooth(stats.sem(array_value_dict, nan_policy='omit'), 10)\n",
    "    STD_array_1st_der = np.diff(STD_array)\n",
    "    STD_array_2nd_der = np.diff(STD_array_1st_der)\n",
    "    return STD_array, STD_array_1st_der, STD_array_2nd_der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract ipsi traces for one width, divide them into up and down and return in concatenated array\n",
    "def return_traces_UP_DOWN_X_width(width):\n",
    "    TA_ipsi_4mm = return_Ipsi_trials_array(width)\n",
    "    TA_ipsi_4mm_DOWN, TA_ipsi_4mm_UP = Divide_TATraces_UpVSDown(TA_ipsi_4mm)\n",
    "    TA_traces_UPDOWN = [TA_ipsi_4mm_DOWN, TA_ipsi_4mm_UP]    \n",
    "    return TA_traces_UPDOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract traces and divide them into IPSI and CONTRA and return in concatenated array\n",
    "def return_traces_IPSI_CONTRA_X_width(width):\n",
    "    TA_ipsi_xmm = return_Ipsi_trials_array(width)\n",
    "    TA_contra_xmm = return_Contra_trials_array(width)\n",
    "    TA_traces_IpsiContra = [TA_ipsi_xmm, TA_contra_xmm]    \n",
    "    return TA_traces_IpsiContra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return TA mean and STD trace, and their 1st and 2nd order derivative in concatenated array\n",
    "def return_MeanSTD_and_derivatives_UpVsDown(TA_traces_UPDOWN):\n",
    "    mean_array = []\n",
    "    STD_array = []\n",
    "    for i in np.arange(len(TA_traces_UPDOWN)):\n",
    "        mean_raw, mean_1stDer, mean_2ndDer = return_mean_and_derivatives(TA_traces_UPDOWN[i])\n",
    "        mean_array.append([mean_raw, mean_1stDer, mean_2ndDer])\n",
    "        STD1_raw, STD_1stDer, STD_2ndDer = return_STD_and_derivatives(TA_traces_UPDOWN[i])\n",
    "        STD_array.append([STD1_raw, STD_1stDer, STD_2ndDer])\n",
    "    return mean_array, STD_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return TA mean and STD trace, and their 1st and 2nd order derivative in concatenated array\n",
    "def return_MeanSTD_and_derivatives_IpsiVsContra(TA_traces_IpsiContra):\n",
    "    mean_array = []\n",
    "    STD_array = []\n",
    "    for i in np.arange(len(TA_traces_IpsiContra)):\n",
    "        mean_raw, mean_1stDer, mean_2ndDer = return_mean_and_derivatives(TA_traces_IpsiContra[i])\n",
    "        mean_array.append([mean_raw, mean_1stDer, mean_2ndDer])\n",
    "        STD1_raw, STD_1stDer, STD_2ndDer = return_STD_and_derivatives(TA_traces_IpsiContra[i])\n",
    "        STD_array.append([STD1_raw, STD_1stDer, STD_2ndDer])\n",
    "    return mean_array, STD_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unpack traces and its derivatives and arrange them such that each array has one Up and one Down traces \n",
    "#(pairing with derivative order)\n",
    "def return_traces_derivatives(mean_array, STD_array):\n",
    "    mean_array_unpacked = [item for sublist in mean_array for item in sublist]\n",
    "    STD_array_unpacked = [item for sublist in STD_array for item in sublist]\n",
    "\n",
    "    mean_array_unpacked_no_der = [mean_array_unpacked[0], mean_array_unpacked[3]]\n",
    "    STD_array_unpacked_no_der = [STD_array_unpacked[0], STD_array_unpacked[3]]\n",
    "    mean_array_unpacked_1st_der = [mean_array_unpacked[1], mean_array_unpacked[4]]\n",
    "    STD_array_unpacked_1st_der = [STD_array_unpacked[1], STD_array_unpacked[4]]\n",
    "    mean_array_unpacked_2nd_der = [mean_array_unpacked[2], mean_array_unpacked[5]]\n",
    "    STD_array_unpacked_2nd_der = [STD_array_unpacked[2], STD_array_unpacked[5]]\n",
    "\n",
    "    mean_array_unpacked_rearranged = [mean_array_unpacked_no_der, mean_array_unpacked_1st_der, mean_array_unpacked_2nd_der]\n",
    "    STD_array_unpacked_rearranged = [STD_array_unpacked_no_der, STD_array_unpacked_1st_der, STD_array_unpacked_2nd_der]\n",
    "    return mean_array_unpacked_rearranged, STD_array_unpacked_rearranged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script to obtain all traces divided into UPs and Down in different column of array\n",
    "width_key = ['4mm', '5mm', '8mm', '10mm']\n",
    "mean_array_unpacked_all = []\n",
    "STD_array_unpacked_all = []\n",
    "\n",
    "for key in width_key:\n",
    "    TA_traces_UPDOWN = return_traces_UP_DOWN_X_width(key)\n",
    "    mean_array, STD_array = return_MeanSTD_and_derivatives_UpVsDown(TA_traces_UPDOWN)\n",
    "    mean_array_unpacked, STD_array_unpacked = return_traces_derivatives(mean_array, STD_array)\n",
    "    mean_array_unpacked_all.append(mean_array_unpacked)\n",
    "    STD_array_unpacked_all.append(STD_array_unpacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Script to obtain all traces divided into Ipsi vs Contra in different column of array\n",
    "width_key = ['4mm', '5mm', '8mm', '10mm']\n",
    "mean_array_unpacked_all_IpsiContra = []\n",
    "STD_array_unpacked_all_IpsiContra = []\n",
    "\n",
    "for key in width_key:\n",
    "    TA_traces_IpsiContra = return_traces_IPSI_CONTRA_X_width(key)\n",
    "    mean_array, STD_array = return_MeanSTD_and_derivatives_IpsiVsContra(TA_traces_IpsiContra)\n",
    "    mean_array_unpacked, STD_array_unpacked = return_traces_derivatives(mean_array, STD_array)\n",
    "    mean_array_unpacked_all_IpsiContra.append(mean_array_unpacked)\n",
    "    STD_array_unpacked_all_IpsiContra.append(STD_array_unpacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in np.arange(4):\n",
    "    plt.plot((mean_array_unpacked_all_IpsiContra[i][0][1]))\n",
    "    plt.plot((STD_array_unpacked_all_IpsiContra[i][0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(4):\n",
    "    plt.plot((mean_array_unpacked_all[i][0][1]))\n",
    "    plt.plot((STD_array_unpacked_all[i][0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot traces color paired based on Ups or Downs\n",
    "_, ax = plt.subplots(figsize=(12, 8))\n",
    "clrs = sns.color_palette(\"husl\", 4)\n",
    "\n",
    "for i in np.arange(4):\n",
    "    plot_PSTH_Mean_STD_label_color_pre_assigned(mean_array_unpacked_all[i][0], STD_array_unpacked_all[i][0], ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot traces color paired based on width\n",
    "_, ax = plt.subplots(figsize=(12, 8))\n",
    "label_strings = [['Down 4mm', 'Up 4mm'], ['Down 5mm', 'Up 5mm'], ['Down 8mm', 'Up 8mm'], ['Down 10mm', 'Up 10mm']]\n",
    "clrs = sns.color_palette(\"husl\", 4)\n",
    "\n",
    "for i in np.arange(4):\n",
    "    plot_PSTH_Mean_STD(mean_array_unpacked_all[i][0], STD_array_unpacked_all[i][0], ax, label_strings[i], clrs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(12, 8))\n",
    "for i in np.arange(4):\n",
    "    plot_PSTH_Mean_STD_label_color_pre_assigned(mean_array_unpacked_all[i][1], STD_array_unpacked_all[i][1], ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(12, 8))\n",
    "for i in np.arange(4):\n",
    "    plot_PSTH_Mean_STD_label_color_pre_assigned(mean_array_unpacked_all[i][2], STD_array_unpacked_all[i][2], ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot traces color paired based on width IPSI VS CONTRA\n",
    "_, ax = plt.subplots(figsize=(12, 8))\n",
    "label_strings = [['Down 4mm', 'Up 4mm'], ['Down 5mm', 'Up 5mm'], ['Down 8mm', 'Up 8mm'], ['Down 10mm', 'Up 10mm']]\n",
    "clrs = sns.color_palette(\"husl\", 4)\n",
    "\n",
    "for i in np.arange(4):\n",
    "    plot_PSTH_Mean_STD(mean_array_unpacked_all_IpsiContra[i][0], STD_array_unpacked_all_IpsiContra[i][0], ax, label_strings[i], clrs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(12, 8))\n",
    "for i in np.arange(4):\n",
    "    plot_PSTH_Mean_STD_label_color_pre_assigned(mean_array_unpacked_all_IpsiContra[i][1], STD_array_unpacked_all_IpsiContra[i][1], ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(12, 8))\n",
    "for i in np.arange(4):\n",
    "    plot_PSTH_Mean_STD_label_color_pre_assigned(mean_array_unpacked_all_IpsiContra[i][2], STD_array_unpacked_all_IpsiContra[i][2], ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
